{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c97169-c047-46b4-a6dd-e25b64608b53",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Start here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7712f7-c892-487e-8386-6188fc0245d6",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "Libraries used in the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c862d959-440d-4664-8873-7467b093efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import inspect\n",
    "from collections.abc import Iterable\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.ensemble import BaseEnsemble\n",
    "\n",
    "# General libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pipelines\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedKFold\n",
    "\n",
    "# Model Selection \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model assessment\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f652-ae2e-4c23-8307-8eaaaca067f2",
   "metadata": {},
   "source": [
    "# Helper Classes\n",
    "\n",
    "These classes are intended to assist in custom transformations of the data that are consistent with the sklearn methodology, by wrapping known functions, such as np.clip, in a fit - transform schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ea97f-779e-401e-a6b4-1922e90ee885",
   "metadata": {},
   "source": [
    "## Wrapper for np.clip()\n",
    "\n",
    "Allows the implementation of np.clip as a custom transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ed9694-c212-4749-93e5-aa84cc2558a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class OutlierClipper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_percentile=0.005, upper_percentile=0.995, use_iqr=False):\n",
    "        \"\"\"\n",
    "        Initialize the OutlierClipper with options for percentile clipping or IQR-based clipping.\n",
    "\n",
    "        Parameters:\n",
    "        - lower_percentile: float, lower bound percentile for clipping (if percentiles are used)\n",
    "        - upper_percentile: float, upper bound percentile for clipping (if percentiles are used)\n",
    "        - use_iqr: bool, whether to use IQR method for determining bounds\n",
    "        \"\"\"\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "        self.use_iqr = use_iqr\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the clipping bounds based on the training dataset using the specified method (percentiles or IQR).\n",
    "\n",
    "        Parameters:\n",
    "        - X: numpy.ndarray or pandas.DataFrame, the dataset used for fitting\n",
    "        - y: ignored, not used for fitting\n",
    "\n",
    "        Returns:\n",
    "        - self: fitted instance of the class\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if input is numpy array\n",
    "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "        \n",
    "        # For each column in X, calculate the bounds using the specified method\n",
    "        self.bounds_ = {}\n",
    "        for column in X.columns:\n",
    "            if self.use_iqr:\n",
    "                q1 = X[column].quantile(0.25)  # 1st quartile\n",
    "                q3 = X[column].quantile(0.75)  # 3rd quartile\n",
    "                iqr = q3 - q1  # Interquartile range\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "            else:\n",
    "                lower_bound = X[column].quantile(self.lower_percentile)\n",
    "                upper_bound = X[column].quantile(self.upper_percentile)\n",
    "\n",
    "            self.bounds_[column] = (lower_bound, upper_bound)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply clipping to the dataset based on the fitted bounds.\n",
    "\n",
    "        Parameters:\n",
    "        - X: numpy.ndarray or pandas.DataFrame, the dataset to transform\n",
    "\n",
    "        Returns:\n",
    "        - X: pandas.DataFrame, the transformed dataset with clipped values\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if input is numpy array\n",
    "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "        # Apply clipping for each column\n",
    "        for column, (lower_bound, upper_bound) in self.bounds_.items():\n",
    "            X[column] = X[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def set_output(self, transform=\"default\"):\n",
    "        \"\"\"\n",
    "        Enable compatibility with scikit-learn's `set_output` functionality.\n",
    "\n",
    "        Parameters:\n",
    "        - transform: str, the output format (\"default\" or \"pandas\").\n",
    "        \"\"\"\n",
    "        self.output_format = transform\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19ea44-fad9-4d4a-8483-821d15698700",
   "metadata": {},
   "source": [
    "# MLAnalytics\n",
    "\n",
    "Responsible for validating instructions, and writting results files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0124cfd1-d6e0-45fd-b700-806dbe710419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLAnalytics:\n",
    "    \"\"\"\n",
    "    A base class to handle shared functionality for analytics-related tasks\n",
    "    like logging and signing pipeline configurations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_dir):\n",
    "        \"\"\"\n",
    "        Initializes common functionality for both signing and logging modes.\n",
    "\n",
    "        Args:\n",
    "            base_dir (str): The base directory that contains subdirectories for logs and results.\n",
    "        \"\"\"\n",
    "        self.base_dir = base_dir\n",
    "        self.config_log_dir = os.path.join(self.base_dir, \"config\")\n",
    "        self.csv_file_path = os.path.join(self.base_dir, \"results\", \"best_runs.csv\")\n",
    "\n",
    "        # Ensure necessary directories exist\n",
    "        self.check_dir_exists(self.config_log_dir, create=True)\n",
    "        self.check_dir_exists(os.path.dirname(self.csv_file_path), create=True)\n",
    "\n",
    "    def check_dir_exists(self, dir_path, create=False):\n",
    "        \"\"\"Ensures that the provided directory path exists. Optionally creates it if necessary based on user input.\"\"\"\n",
    "        if dir_path and not os.path.exists(dir_path):\n",
    "            if create:\n",
    "                response = input(f\"Directory '{dir_path}' does not exist. Do you want to create it? (y/n): \").strip().lower()\n",
    "                if response == 'y':\n",
    "                    # Case when it is okay to create directory, and choice is yes.\n",
    "                    os.makedirs(dir_path, exist_ok=True)\n",
    "                    print(f\"Directory '{dir_path}' has been created.\")\n",
    "                else:\n",
    "                    # Case when it is okay to create directory, but choice is no.\n",
    "                    print(f\"Directory '{dir_path}' was not created.\")\n",
    "                    return False\n",
    "            else:\n",
    "                # Case when it is not allowed to create a directory and none valid was given.\n",
    "                raise FileNotFoundError(f\"No directory found at {dir_path}\")\n",
    "        # Case when existing directory is provided.\n",
    "        return True\n",
    "\n",
    "    def get_target_dir_current_id(self, target_dir):\n",
    "        \"\"\"\n",
    "        Returns the current available run ID by checking the run log directory.\n",
    "\n",
    "        Returns:\n",
    "            int: The current available run ID.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.check_dir_exists(target_dir, create=True):\n",
    "        \n",
    "            existing_files = os.listdir(target_dir)\n",
    "            target_ids = []\n",
    "            for f in existing_files:\n",
    "                if f.endswith('.json'):\n",
    "                    try:\n",
    "                        target_id = int(f.split('_')[-1].replace('.json', ''))\n",
    "                        target_ids.append(target_id)\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping file with unexpected name format: {f}\")\n",
    "\n",
    "            return max(target_ids, default=0) + 1\n",
    "\n",
    "    def get_target_dir_target_id(self, target_dir=None, target_id=None):\n",
    "        if not target_dir and target_id:\n",
    "            raise ValueError(\"Must provide directory and id, to fetch.\")\n",
    "\n",
    "        filename = f\"{target_dir}_id_{target_id}\"\n",
    "        \n",
    "        target_dir = os.path.join(self.base_dir, target_dir)\n",
    "\n",
    "        self.check_dir_exists(target_dir, create=False)\n",
    "        \n",
    "        try:\n",
    "            # Construct the filename\n",
    "            filename = f\"{target_dir}/{filename}.json\"\n",
    "            with open(filename, \"r\") as f:\n",
    "                instructions_json = json.load(f)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"No configuration file found at {filename}\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(f\"Failed to decode JSON in the file at {filename}\")\n",
    "\n",
    "        return instructions_json\n",
    "\n",
    "class MLAuditer(MLAnalytics):\n",
    "    \"\"\"Subclass of MLAnalytics for signing and storing pipeline configurations.\"\"\"\n",
    "\n",
    "    def sign_and_save_config(self, pipeline_config):\n",
    "        \"\"\"\n",
    "        Signs a pipeline configuration with a run_id and saves it as JSON.\n",
    "        \n",
    "        Args:\n",
    "            pipeline_config (dict): The pipeline configuration to sign and save.\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to the saved JSON file.\n",
    "        \"\"\"\n",
    "        signed_config = pipeline_config.copy()\n",
    "        signed_config[\"config_id\"] = self.get_target_dir_current_id(self.config_log_dir)  # Generate a config ID here\n",
    "\n",
    "        json_file = os.path.join(self.config_log_dir, f\"config_id_{signed_config['config_id']}.json\")\n",
    "        try:\n",
    "            with open(json_file, \"w\") as f:\n",
    "                json.dump(signed_config, f, indent=4)\n",
    "            print(f\"Signed pipeline configuration saved at: {json_file}\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving configuration: {e}\")\n",
    "            raise\n",
    "\n",
    "        return json_file\n",
    "\n",
    "\n",
    "class MLLogger(MLAnalytics):\n",
    "    \"\"\"Subclass of MLAnalytics for logging and storing results.\"\"\"\n",
    "\n",
    "    def __init__(self, base_dir, instructions_json):\n",
    "        \"\"\"\n",
    "        Initializes the logger with directory and instructions.\n",
    "        \n",
    "        Args:\n",
    "            base_dir (str): Base directory for saving results.\n",
    "            instructions_json (dict): The instructions containing configuration details.\n",
    "        \"\"\"\n",
    "        super().__init__(base_dir)\n",
    "        self.config_id = instructions_json.get(\"config_id\")  # Retrieve config_id from instructions\n",
    "\n",
    "    def results_to_csv(self, result_data):\n",
    "        \"\"\"\n",
    "        Appends results to a CSV file, injecting the config_id and run_id.\n",
    "        \n",
    "        Args:\n",
    "            result_data (list of dicts): The result data to append to the CSV.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if not result_data:\n",
    "            raise ValueError(\"No result data to save.\")\n",
    "\n",
    "        df = pd.DataFrame(result_data)\n",
    "        df['config_id'] = self.config_id\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(self.csv_file_path):\n",
    "                df.to_csv(self.csv_file_path, index=False)\n",
    "            else:\n",
    "                df.to_csv(self.csv_file_path, mode=\"a\", header=False, index=False)\n",
    "            print(f\"Results saved to {self.csv_file_path}\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving results to CSV: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b7810-d9dc-4931-a1f5-72fd967a44b8",
   "metadata": {},
   "source": [
    "# PipelineBuilder\n",
    "\n",
    "Pipelinebuilder assists with the creation of pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de9d51c-b5a6-44df-9545-37bb289d0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineBuilder:\n",
    "    \"\"\"\n",
    "    Base class for building machine learning pipelines.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_names=None, auditer=None):\n",
    "        if self.__class__ is PipelineBuilder:\n",
    "            raise TypeError(\"Cannot instantiate abstract class PipelineBuilder.\")\n",
    "\n",
    "        self.column_names = column_names or []  # Store column names if provided\n",
    "        self._blueprints = {}  # Dictionary to hold pipeline instructions\n",
    "                \n",
    "        if not auditer:\n",
    "            raise ValueError(\"Cannot run PipelineBuiler without an instance of auditer\")\n",
    "\n",
    "        self.auditer = auditer\n",
    "    \n",
    "    def _inspect_pipelines(self):\n",
    "        \"\"\"\n",
    "        Display all pipelines stored in the _blueprints.\n",
    "        \"\"\"\n",
    "        if not self._blueprints:\n",
    "            print(\"No pipelines have been created yet.\")\n",
    "        else:\n",
    "            print(\"Current Pipelines:\")\n",
    "            for name, steps in self._blueprints.items():\n",
    "                print(f\"Pipeline '{name}':\")\n",
    "                for idx, step in enumerate(steps['sections'], 1):\n",
    "                    print(f\"  {idx}. {step}\")\n",
    "    \n",
    "        eligible_params = self._get_class_params(section_class)\n",
    "        return True, section_class, eligible_params\n",
    "\n",
    "    def _get_class_params(self, section_class):\n",
    "        \"\"\"\n",
    "        Retrieves eligible parameters for a class using inspection.\n",
    "        \"\"\"\n",
    "        params = inspect.signature(section_class).parameters\n",
    "        return list(params.keys())\n",
    "    \n",
    "    def _retrieve_class_from_scope(self, section_name):\n",
    "        \"\"\"\n",
    "        Retrieves the class object from global or local scope.\n",
    "        \"\"\"\n",
    "        section_class = globals().get(section_name) or locals().get(section_name)\n",
    "        if section_class is None:\n",
    "            print(f\"Error: '{section_name}' is not defined in the current scope. Try again.\")\n",
    "            return None\n",
    "        if not callable(section_class):\n",
    "            print(f\"Error: '{section_name}' is not callable. Try again.\")\n",
    "            return None\n",
    "        return section_class\n",
    "        \n",
    "    def _validate_section(self, section_name, is_predictor):\n",
    "        \"\"\"\n",
    "        Validates if a given section can be added to the pipeline.\n",
    "        \"\"\"\n",
    "        section_class = self._retrieve_class_from_scope(section_name)\n",
    "        if not section_class:\n",
    "            return False, None, []\n",
    "    \n",
    "        if is_predictor:\n",
    "            valid, section_class  = self._validate_predictor(section_class)\n",
    "        else:\n",
    "            valid, section_class = self._validate_transformer(section_class)\n",
    " \n",
    "        if not valid:\n",
    "            return False, None, []\n",
    "        else:\n",
    "            valid_params = self._get_class_params(section_class)\n",
    "            return valid, section_class, valid_params\n",
    "            \n",
    "    def _validate_predictor(self, section_class):\n",
    "        \"\"\"\n",
    "        Validates if the given class is a predictor.\n",
    "        \"\"\"\n",
    "        if not hasattr(section_class, 'predict') or \\\n",
    "           not any(base.__name__ in ['BaseEstimator', 'BaseEnsemble'] for base in inspect.getmro(section_class)):\n",
    "            print(f\"Error: '{section_class.__name__}' is not a valid predictor \"\n",
    "                  f\"(no 'predict' method or not derived from 'BaseEstimator' or 'BaseEnsemble'). Try again.\")\n",
    "            return False, None\n",
    "        return True, section_class\n",
    "\n",
    "    def _validate_transformer(self, section_class):\n",
    "        \"\"\"\n",
    "        Validates if the given class is a transformer.\n",
    "        \"\"\"\n",
    "        if not hasattr(section_class, 'transform') or \\\n",
    "           'TransformerMixin' not in [base.__name__ for base in inspect.getmro(section_class)]:\n",
    "            print(f\"Error: '{section_class.__name__}' is not a valid transformer \"\n",
    "                  f\"(no 'transform' method or not derived from 'TransformerMixin'). Try again.\")\n",
    "            return False, None\n",
    "        return True, section_class\n",
    "\n",
    "class IterPlumber(PipelineBuilder):\n",
    "    \"\"\"\n",
    "    Interactive pipeline builder allowing step-by-step creation and management of multiple pipelines.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_names=None, auditer=None):\n",
    "        if auditer is None:\n",
    "            raise ValueError(\"IterPlumber requires an instance of auditer to be provided.\")\n",
    "        super().__init__(column_names=column_names, auditer=auditer)\n",
    "\n",
    "        # Initialize available sections as an empty \n",
    "        self.available_sections = {\n",
    "            'transformers': {}, \n",
    "            'predictors' : {}\n",
    "        } # Tracks user-created components not yet in use\n",
    "        \n",
    "    def run_pipes(self):\n",
    "        \"\"\"\n",
    "        Main method to manage the creation of pipeline instructions for multiple pipelines iteratively.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            user_input = input(\n",
    "                \"\"\"\n",
    "    Welcome to IterPlumber! Please select an option:\n",
    "    1. Create a new pipeline\n",
    "    2. View current pipelines\n",
    "    3. Finalize blueprints\n",
    "    4. Cancel and exit\n",
    "    Your choice: \"\"\".strip()).strip()\n",
    "    \n",
    "            if user_input == '1':\n",
    "                # Initialize the current pipeline as an empty dictionary\n",
    "                print(\"\\nFollow the instructions to iteratively build a pipeline.\")\n",
    "                current_pipeline = self._build_pipeline()\n",
    "    \n",
    "                # If a pipeline was successfully built\n",
    "                if current_pipeline:\n",
    "                    pipeline_name = current_pipeline['name']\n",
    "                    self._blueprints[pipeline_name] = current_pipeline\n",
    "                    print(f\"Pipeline '{pipeline_name}' saved successfully.\")\n",
    "                else:\n",
    "                    print(\"Pipeline creation was cancelled or not completed.\")\n",
    "    \n",
    "            elif user_input == '2':\n",
    "                # View current pipelines\n",
    "                if self._blueprints:\n",
    "                    print(\"\\nCurrent pipelines:\")\n",
    "                    for name, pipeline in self._blueprints.items():\n",
    "                        print(f\"- {name}: {pipeline}\")\n",
    "                else:\n",
    "                    print(\"\\nNo pipelines have been created yet.\")\n",
    "    \n",
    "            elif user_input == '3':\n",
    "                # Finalize and save the pipelines\n",
    "                print(\"\\nFinalizing and saving pipelines...\")\n",
    "                self._finalize_blueprints()\n",
    "                print(\"Pipelines finalized successfully. Goodbye!\")\n",
    "                break\n",
    "    \n",
    "            elif user_input == '4':\n",
    "                # Cancel and exit\n",
    "                print(\"\\nCancelling all operations. Goodbye!\")\n",
    "                break\n",
    "    \n",
    "            else:\n",
    "                # Handle invalid input\n",
    "                print(\"\\nInvalid choice. Please select a valid option (1-4).\")\n",
    "\n",
    "\n",
    "    def _build_pipeline(self):\n",
    "        \"\"\"\n",
    "        Helper method to interactively build pipeline instructions for a single pipeline.\n",
    "        \"\"\"\n",
    "        self.current_column_names = {col: col for col in self.column_names}  # Keeps column mapping updated dynamically\n",
    "        \n",
    "        # Empty pipeline is instanced\n",
    "        section_id = 0\n",
    "        section_number = 0\n",
    "        \n",
    "        this_pipeline = {\n",
    "            'name': ''\n",
    "            , 'n_sections': 0\n",
    "            , 'sections': {}\n",
    "        }\n",
    "\n",
    "        # Flow controls for pipeline assembly\n",
    "        has_transformer = False\n",
    "        has_predictor = False\n",
    "    \n",
    "        while True:\n",
    "            print(\"\\n--- Pipeline Creation Menu ---\")\n",
    "            print(\"To build a pipeline, add at least one transformer and one predictor section.\"\n",
    "                  \"The last section must always be a predictor. Sections must be built before they can be added.\")\n",
    "            print(\"\\nOptions:\")\n",
    "            print(\"1. Build a transformer section\")\n",
    "            print(\"   - A transformer applies preprocessing steps to your data, such as scaling, encoding, or imputation.\")\n",
    "            print(\"2. Build a predictor\")\n",
    "            print(\"   - A predictor is the final step in the pipeline, such as a regression model or classifier.\")\n",
    "            print(\"3. Add a section to the pipeline\")\n",
    "            print(\"   - Use this to integrate a previously defined transformer or predictor into the pipeline.\")\n",
    "            print(\"4. View current pipeline\")\n",
    "            print(\"   - Displays the steps currently added to this pipeline, in the order they will be applied.\")\n",
    "            print(\"5. View current available sections\")\n",
    "            print(\"   - Displays the steps currently added to this pipeline, in the order they will be applied.\")\n",
    "            print(\"6. Finish and save this pipeline\")\n",
    "            print(\"   - Completes the pipeline creation process and saves the current pipeline.\")\n",
    "            print(\"7. Cancel this pipeline\")\n",
    "            print(\"   - Discards the current pipeline and returns to the main menu.\")\n",
    "            \n",
    "            choice = input(\"Enter your choice: \").strip()\n",
    "            \n",
    "            if choice == '1':\n",
    "                print(\"Building transformer section.\")\n",
    "                self._handle_section()\n",
    "        \n",
    "            elif choice == '2':\n",
    "                print(\"Building predictor.\")\n",
    "                self._handle_section(is_predictor=True)\n",
    "                \n",
    "            elif choice == '3':\n",
    "                if has_predictor:\n",
    "                    print('Unable to add more sections, pipeline already has a predictor')\n",
    "                    break\n",
    "                \n",
    "                if self.available_sections['transformers'] or self.available_sections['predictors']:\n",
    "                    while True:\n",
    "                        print(\"Adding section to the pipeline.\")\n",
    "                        print(\"Select which type of section you wish to add to the pipeline\")\n",
    "                        print(\"1. Add a transformer\")\n",
    "                        print(\"2. Add a predictor\")\n",
    "                        print(\"3. Add a column_transformer (requires a transformer)\")\n",
    "                        print(\"4. Go back to Pipeline creation menu.\")\n",
    "    \n",
    "                        self._view_sections()\n",
    "                        \n",
    "                        choice = input(\"Enter your choice: \").strip()\n",
    "                        if choice == '1':\n",
    "                            if self.available_sections['transformers']:\n",
    "                                has_transformer = self._add_section(this_pipeline, is_predictor=False)\n",
    "                            else:\n",
    "                                print('No transfomers available to add.')\n",
    "                            \n",
    "                        elif choice == '2':\n",
    "                            if self.available_sections['predictors']:\n",
    "                                has_predictor = self._add_section(this_pipeline, is_predictor=True)\n",
    "                            else:\n",
    "                                print('No predictors available to add.')\n",
    "                        elif choice == '3':\n",
    "                            if self.available_sections['transformers']:\n",
    "                                has_transformer = self._add_section(this_pipeline, is_column_transformer=True)\n",
    "\n",
    "                        elif choice == '4':\n",
    "                            print('Returning to previous menu.')\n",
    "                            break\n",
    "                        else:\n",
    "                            print('No transfomers available to add.')\n",
    "                            \n",
    "                else:\n",
    "                    print(\"No pipes available to add.\")\n",
    "                \n",
    "            elif choice == '4':\n",
    "                self._view_pipeline(this_pipeline)\n",
    "                \n",
    "            elif choice == '5':\n",
    "                self._view_sections()\n",
    "                \n",
    "            elif choice == '6':\n",
    "                if has_predictor:\n",
    "                    predictor_key = max(this_pipeline['sections'].keys())\n",
    "                    this_pipeline['name'] = this_pipeline['sections'][predictor_key]['name']\n",
    "                \n",
    "                    print(\"Saving the current pipeline with predictor:\", this_pipeline['name'] )\n",
    "                    return this_pipeline\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Cannot store empty or incomplete pipeline blueprints. Please try again.\")\n",
    "                    \n",
    "            elif choice == '7': \n",
    "                print(\"Cancelling the current pipeline and returning to the main menu.\")\n",
    "                input(\"Press any key to continue\")\n",
    "                return None\n",
    "                \n",
    "            else:\n",
    "                print(\"Invalid choice. Please try again.\")\n",
    "            \n",
    "            input(\"Press any key to continue\")\n",
    "        \n",
    "                \n",
    "    def _handle_section(self, is_predictor=False):\n",
    "        \"\"\"Handles flow into _build_section which itself provides an iterative procedure to generate pipelines, receives a section in return and parses it into the available_sections\"\"\"\n",
    "        if is_predictor:\n",
    "            string = \"predictors\"\n",
    "        else:\n",
    "            string = \"transformers\"\n",
    "        \n",
    "        section_name = input(f\"Enter the {string} class name: \").strip()   \n",
    "\n",
    "        if section_name.lower() in self.available_sections[string]:\n",
    "            print(f'Failed to create {section_name}. Another section with that name already exists')\n",
    "            valid = False\n",
    "        else:\n",
    "            valid, section_class, eligible_params = self._validate_section(section_name, is_predictor)\n",
    "        \n",
    "        if valid:\n",
    "            section = self._build_section(section_name, section_class, eligible_params)\n",
    "\n",
    "            self.available_sections[string][section['_name']] = {\n",
    "                'name': section['_name'],\n",
    "                'class': section['_class'],\n",
    "                'args': section['_args'],\n",
    "                'grid': section['_grid'],\n",
    "                'columns' : section['_columns']\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            input(f\"Failed to build {string} section. Press any key to return to the previous menu\")\n",
    "                \n",
    "    def _build_section(self, section_name, section_class, eligible_params):\n",
    "        \"\"\"\n",
    "        Prompts the user to provide values for eligible parameters of a section\n",
    "        and validates them by instantiating the class and calling its `fit` method.\n",
    "        \"\"\"\n",
    "        # Try to print documentation to assist the user\n",
    "        try:\n",
    "            print(section_class.__doc__)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nUnexpected issue with docstring: {e}\")\n",
    "    \n",
    "        print(f\"Now building '{section_name.lower()}'. Ensure that parameter values are correct (Docstring above).\")\n",
    "    \n",
    "        raw_params, params = {}, {}\n",
    "        raw_grid_params, grid_params = {}, {}\n",
    "         \n",
    "\n",
    "        for param in eligible_params:\n",
    "            user_input = input(f\"Enter a value for '{param}' (or an iterable like a list or range for grid search if applicable): \")\n",
    "            try:\n",
    "                if user_input == '':\n",
    "                    user_input = None\n",
    "                else:\n",
    "                    parsed_input = eval(user_input)\n",
    "            except (ValueError, SyntaxError, NameError):\n",
    "                parsed_input = user_input\n",
    "        \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if isinstance(parsed_input, (dict, list, Iterable)) and not isinstance(parsed_input, str):\n",
    "                raw_grid_params[param], grid_params[param] = user_input, parsed_input\n",
    "            else:\n",
    "                raw_params[param], params[param] = parsed_input, parsed_input\n",
    "                \n",
    "        if not grid_params:\n",
    "                max_iterations = 1\n",
    "                grid_iterators = {}\n",
    "        else:\n",
    "            grid_iterators = {param: iter(values) for param, values in grid_params.items()}\n",
    "                 \n",
    "        # Testing loop\n",
    "        number_iterators = len(grid_params.keys())\n",
    "        number_stop_iterators = 0\n",
    "        current_args = params.copy()\n",
    "        \n",
    "        try:\n",
    "            while number_stop_iterators != number_iterators:\n",
    "                number_stop_iterators = 0\n",
    "                for param, iterator in grid_iterators.items():\n",
    "                    try:\n",
    "                        current_args[param] = next(iterator)\n",
    "                    except StopIteration:\n",
    "                        number_stop_iterators += 1\n",
    "                    \n",
    "                print(f\"Testing with arguments: {current_args}\")\n",
    "                dummy_data_x, dummy_data_y = [[0]], [[0]]  # Placeholder data\n",
    "                section_instance = section_class(**current_args)\n",
    "                try:\n",
    "                    section_instance.fit(dummy_data_x)  # Validate the instance\n",
    "                except Exception as e:\n",
    "                    section_instance.fit(dummy_data_x, dummy_data_y)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed with arguments {current_args}: {e}\")\n",
    "            retry = input(\"Invalid arguments detected, try again? (y/n): \").strip().lower()\n",
    "            if retry != 'y':\n",
    "                print(\"Exiting pipeline creation.\")\n",
    "                return None\n",
    "            else:\n",
    "                # Retry the entire section\n",
    "                return self._build_section(section_name, section_class, eligible_params)\n",
    "    \n",
    "        print(f\"Successfully tested all arguments,\")\n",
    "        section = {\n",
    "            '_name': section_class.__name__.lower(),\n",
    "            '_class': section_class.__name__,\n",
    "            '_args': raw_params,\n",
    "            '_grid': raw_grid_params,\n",
    "            '_columns': []\n",
    "        }\n",
    "    \n",
    "        return section\n",
    "\n",
    "    \n",
    "    def _add_section(self, pipeline, is_predictor=False, is_column_transformer=False):\n",
    "        \"\"\"\n",
    "        Add a section to the pipeline, either as a predictor or a transformer.\n",
    "        Handles column selection for column transformers iteratively.\n",
    "    \n",
    "        Args:\n",
    "            pipeline (dict): The current pipeline being built.\n",
    "            is_predictor (bool): Whether the section is a predictor. Defaults to False.\n",
    "            is_column_transformer (bool): Whether the section is a column transformer. Defaults to False.\n",
    "    \n",
    "        Returns:\n",
    "            bool: True if at least one section was added, False otherwise.\n",
    "        \"\"\"\n",
    "        section_type = 'predictors' if is_predictor else 'transformers'\n",
    "        selectable_sections = self.available_sections[section_type].copy()\n",
    "        selected_sections = {}\n",
    "        \n",
    "        # For column transformers, maintain a list of selectable columns\n",
    "        selectable_columns = self.column_names.copy() if is_column_transformer else None\n",
    "        has_more_to_add = True\n",
    "        at_least_one_selected = False\n",
    "        section_number = 0  # Internal section number (increments for column transformers)\n",
    "    \n",
    "        while has_more_to_add:\n",
    "            # Step 1: Handle column selection if necessary\n",
    "            selected_columns = None\n",
    "            if is_column_transformer:\n",
    "                selected_columns = self._select_columns(selectable_columns)\n",
    "                if not selected_columns:\n",
    "                    print(\"No columns selected. Returning to the previous menu.\")\n",
    "                    return False\n",
    "                # Update the remaining selectable columns\n",
    "                selectable_columns = [col for col in selectable_columns if col not in selected_columns]\n",
    "    \n",
    "            # Step 2: Handle section selection\n",
    "            section_name, section_details = self._select_section(selectable_sections)\n",
    "            if not section_name:\n",
    "                print(\"No section selected. Returning to the previous menu.\")\n",
    "                return False\n",
    "                        \n",
    "            # Add columns if this is a column transformer\n",
    "            if is_column_transformer:\n",
    "                section_details['name'] = section_name\n",
    "                section_details['columns'] = selected_columns\n",
    "                print(f\"Assigned columns {selected_columns} to {section_name}.\")\n",
    "                \n",
    "            \n",
    "            # Increment for each section selected\n",
    "            section_number += 1  \n",
    "            \n",
    "            # Store the selected section\n",
    "            selected_sections[section_number] = section_details\n",
    "            selectable_sections.pop(section_name, None)  # Remove from available sections\n",
    "            \n",
    "            at_least_one_selected = True\n",
    "    \n",
    "            # Step 3: Decide whether to add more transformers to this column transformer\n",
    "            if is_column_transformer and selectable_columns and selectable_sections:\n",
    "                add_more = input(\"Add another transformer to the column transformer? (y/n): \").strip().lower()\n",
    "                if add_more != 'y':\n",
    "                    print(\"Finalizing current column transformer.\")\n",
    "                    has_more_to_add = False\n",
    "            else:\n",
    "                has_more_to_add = False  # No more sections or columns to add\n",
    "    \n",
    "        if at_least_one_selected:\n",
    "            # Update the id before incrementing\n",
    "            section_id = pipeline['n_sections'] + 1\n",
    "                \n",
    "            # Step 4: Add column transformer or section to the pipeline\n",
    "            if is_column_transformer:\n",
    "                # Create a composite name for the column transformer\n",
    "                transformer_name = '_'.join(\n",
    "                    ['column_transformer'] + [\n",
    "                        selected_sections[section_number]['name'] for section_number in selected_sections\n",
    "                    ]\n",
    "                )\n",
    "                column_transformer = {\n",
    "                    'name': transformer_name,\n",
    "                    'transformers': selected_sections\n",
    "                }\n",
    "                # Add the column transformer to the pipeline\n",
    "                pipeline['sections'][section_id] = column_transformer\n",
    "\n",
    "            else:\n",
    "                # Add single predictor/transformer to the pipeline\n",
    "                section_number = next(iter(selected_sections))  # Extract the single selected section\n",
    "                pipeline['sections'][section_id] = selected_sections[section_number]\n",
    "            \n",
    "            # Remove selected sections from available_sections\n",
    "            for section_number in selected_sections:\n",
    "                del self.available_sections[section_type][selected_sections[section_number]['name']]\n",
    "\n",
    "            # Increment the number of sections\n",
    "            pipeline['n_sections'] += 1\n",
    "        \n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(\"No sections were successfully added to the pipeline.\")\n",
    "            return False\n",
    "\n",
    "    def _select_section(self, available_sections):\n",
    "        while True:\n",
    "            print(f\"Available sections: \\n{available_sections.keys()}\")\n",
    "            chosen_section = input(\"Select a transformer by entering its name (see above)\")\n",
    "            \n",
    "            if chosen_section not in available_sections.keys():\n",
    "                print(\"Failed to match your input with an available section\")\n",
    "                retry = input(\"Try again y/n?\")\n",
    "                if retry == 'y':\n",
    "                    continue\n",
    "                else:\n",
    "                    print('Returning to the previous menu')\n",
    "                    return None, None\n",
    "            else:\n",
    "                return chosen_section, available_sections[chosen_section]\n",
    "                    \n",
    "    def _select_columns(self, remaining_columns):\n",
    "        \"\"\"\n",
    "        Prompt the user to select specific columns for the given transformer section using indices,\n",
    "        ranges, or slices. Allows the user to cancel and return to the previous menu.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Display available columns with indices\n",
    "            print(\"\\nAvailable columns:\")\n",
    "            for idx, col in enumerate(remaining_columns, start=1):\n",
    "                print(f\"{idx}. {col}\")\n",
    "            \n",
    "            print(\"\\nEnter column indices, ranges (e.g., range(1, 5)), or slices (e.g., 1:5).\")\n",
    "            print(\"Type 'cancel' to return to the previous menu.\")\n",
    "            \n",
    "            # Get user input\n",
    "            user_input = input(\"Enter indices, ranges, or 'cancel': \").strip()\n",
    "            \n",
    "            # Handle cancel option\n",
    "            if user_input.lower() == 'cancel':\n",
    "                print(\"Returning to the previous menu.\")\n",
    "                return None\n",
    "            \n",
    "            try:\n",
    "                # Try to evaluate the entire input as an iterable\n",
    "                selected_indices = []\n",
    "                try:\n",
    "                    evaluated = eval(user_input)\n",
    "                    if isinstance(evaluated, Iterable):\n",
    "                        selected_indices.extend(list(evaluated))\n",
    "                    else:\n",
    "                        raise ValueError(\"Input is not an iterable.\")\n",
    "                except Exception:\n",
    "                    # If not an iterable, split and process parts individually\n",
    "                    inputs = [item.strip() for item in user_input.split(',')]\n",
    "                    for item in inputs:\n",
    "                        selected_indices.extend(list(eval(item)))\n",
    "                \n",
    "                # Deduplicate and validate indices\n",
    "                selected_indices = sorted(set(selected_indices))  # Remove duplicates\n",
    "                if not all(1 <= idx <= len(remaining_columns) for idx in selected_indices):\n",
    "                    print(f\"Invalid indices. Please ensure values are between 1 and {len(remaining_columns)}.\")\n",
    "                    continue\n",
    "                \n",
    "                # Translate indices to column names\n",
    "                selected_columns = [remaining_columns[idx - 1] for idx in selected_indices]\n",
    "                \n",
    "                # Return the selected columns\n",
    "                return selected_columns\n",
    "            except Exception as e:\n",
    "                print(f\"Invalid input: {e}. Please enter valid indices, ranges, or slices.\")\n",
    "    \n",
    "    def _view_sections(self):\n",
    "        for section_type in self.available_sections.items():\n",
    "            if not section_type:\n",
    "                print(f\"No sections available for type {section_type}\")\n",
    "            else:\n",
    "                for section in section_type:\n",
    "                    print(section),\n",
    "                print()  \n",
    "                \n",
    "    def _view_pipeline(self, this_pipeline):\n",
    "        if not this_pipeline:\n",
    "            print(\" No sections in use/available.\")\n",
    "        else:\n",
    "            print(\"Current pipeline:\")\n",
    "            for step_name, step_details in this_pipeline['sections'].items():\n",
    "                print(f\"- {step_name}: {step_details}\")\n",
    "\n",
    "    def _finalize_blueprints(self):\n",
    "        \"\"\"\n",
    "        Finalize all blueprints by saving them using an external auditor.\n",
    "        \"\"\"\n",
    "        if not self._blueprints:\n",
    "            print(\"No pipelines were created. Nothing to save.\")\n",
    "            return\n",
    "    \n",
    "        print(\"\\nFinalizing and saving all created pipelines...\")\n",
    "        # Call auditer to sign the blueprint to the working directory\n",
    "        self.auditer.sign_and_save_config(self._blueprints)\n",
    "        print(\"Pipelines saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d384007-2b02-44c8-bd87-cc00c212b0fa",
   "metadata": {},
   "source": [
    "# Fitter\n",
    "\n",
    "Trains models using instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e579b9d-65e4-428e-bed7-0e220c4192bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter:\n",
    "    \n",
    "    def __init__(self, X, y, instructions_json, logger):\n",
    "        \n",
    "        if not instructions_json or not logger:\n",
    "            raise ValueError(\"Both 'instructions_json' and 'logger' must be provided.\")\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.ml_logger = logger\n",
    "        \n",
    "        # Parse blueprints into pipelines and grids\n",
    "        self.instructions = self.lay_pipeline(instructions_json.copy())\n",
    "\n",
    "        print(self.instructions)\n",
    "        \n",
    "        self.outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        self.inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        self.results_buffer = []\n",
    "        \n",
    "    def lay_pipeline(self, json_blueprints):\n",
    "        pipelines_and_grids = []\n",
    "        column_mapping = {col: col for col in X_train.columns}  # Initial mapping\n",
    "        \n",
    "        for blueprint_name, blueprint_data in json_blueprints.items():\n",
    "            if blueprint_name == 'config_id':\n",
    "                continue\n",
    "            \n",
    "            sections = blueprint_data['sections']\n",
    "            steps = []\n",
    "            grid_params = {}\n",
    "    \n",
    "            for section_id, section_info in sections.items():\n",
    "                print(section_id)\n",
    "                if 'transformers' in section_info:  # Column Transformer\n",
    "                    column_transformer_steps = []\n",
    "                    for transformer in section_info['transformers'].values():\n",
    "                        # Evaluate arguments for the transformer\n",
    "                        for key, value in transformer['args'].items():\n",
    "                            try:\n",
    "                                evaluated_value = eval(value)\n",
    "                                if isinstance(evaluated_value, object):\n",
    "                                    transformer['args'][key] = evaluated_value\n",
    "                            except:\n",
    "                                pass  # If evaluation fails, retain the original value\n",
    "                        \n",
    "                        # Replace column names with mapped names\n",
    "                        transformer_columns = [column_mapping.get(col, col) for col in transformer['columns']]\n",
    "    \n",
    "                        # Transformer step details\n",
    "                        transformer_name = transformer['name']\n",
    "                        transformer_class = eval(transformer['class'])(**transformer['args'])\n",
    "    \n",
    "                        # Add transformer to ColumnTransformer steps\n",
    "                        column_transformer_steps.append((transformer_name, transformer_class, transformer_columns))\n",
    "    \n",
    "                        # Extract grid parameters for the transformer\n",
    "                        for param_key, param_values in transformer['grid'].items():\n",
    "                            grid_key = f\"{section_info['name']}__{transformer_name}__{param_key}\"\n",
    "                            grid_params[grid_key] = eval(param_values)\n",
    "    \n",
    "                    # Add the ColumnTransformer to the pipeline and ensure pandas output\n",
    "                    column_transformer = ColumnTransformer(column_transformer_steps, remainder='passthrough')\n",
    "                    column_transformer.set_output(transform=\"pandas\")\n",
    "                    \n",
    "                    # Add the fitted column transformer to the pipeline\n",
    "                    steps.append((section_info['name'], column_transformer))\n",
    "                    \n",
    "                else:  # Model/Predictor\n",
    "                    # Evaluate arguments for the model\n",
    "                    for key, value in section_info['args'].items():\n",
    "                        try:\n",
    "                            evaluated_value = eval(value)\n",
    "                            if isinstance(evaluated_value, object):\n",
    "                                section_info['args'][key] = evaluated_value\n",
    "                        except:\n",
    "                            pass  # If evaluation fails, retain the original value\n",
    "    \n",
    "                    step_name = section_info['name']\n",
    "                    model_instance = eval(section_info['class'])(**section_info['args'])\n",
    "                    steps.append((step_name, model_instance))\n",
    "    \n",
    "                    # Extract grid parameters for the model\n",
    "                    for param_key, param_values in section_info['grid'].items():\n",
    "                        grid_key = f\"{step_name}__{param_key}\"\n",
    "                        grid_params[grid_key] = eval(param_values)\n",
    "    \n",
    "            # Create the pipeline and ensure pandas output\n",
    "            pipeline = Pipeline(steps)\n",
    "            pipeline.set_output(transform=\"pandas\")\n",
    "            pipelines_and_grids.append((blueprint_name, pipeline, grid_params))\n",
    "    \n",
    "        return pipelines_and_grids\n",
    "\n",
    "    def process_and_store_results(self, model_name, df_results, best_fold_index):\n",
    "        \"\"\"\n",
    "        Processes cross-validation results and stores them in a structured format.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the model.\n",
    "            df_results (dict): Cross-validation results.\n",
    "            best_fold_index (int): Index of the best scoring fold.\n",
    "\n",
    "        Returns:\n",
    "            dict: Processed results for the model.\n",
    "        \"\"\"\n",
    "        best_fold_estimator = df_results['estimator'][best_fold_index]\n",
    "        best_score = df_results['test_score'][best_fold_index]\n",
    "        best_params = best_fold_estimator.best_params_\n",
    "        best_fold_estimator.fit(self.X, self.y)\n",
    "        refit_train_score = best_fold_estimator.score(self.X, self.y)\n",
    "        \n",
    "        mean_train_score = df_results['train_score'].mean()\n",
    "        mean_test_score = df_results['test_score'].mean()\n",
    "        fit_time = df_results['fit_time'][best_fold_index]\n",
    "        score_time = df_results['score_time'][best_fold_index]\n",
    "\n",
    "        model_result = {\n",
    "            \"model\": model_name,\n",
    "            \"best_hyperparameters\": best_params,\n",
    "            \"best_test_score\": best_score,\n",
    "            \"mean_train_score\": mean_train_score,\n",
    "            \"mean_test_score\": mean_test_score,\n",
    "            \"fit_time\": fit_time,\n",
    "            \"score_time\": score_time,\n",
    "            \"refit_train_score\": refit_train_score,\n",
    "        }\n",
    "\n",
    "        self.results_buffer.append(model_result)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executes the fitting process for all instructions and handles results storage.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for model_name, pipeline, params in self.instructions:     \n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=pipeline,\n",
    "                    param_grid=params,\n",
    "                    scoring='f1_macro',\n",
    "                    cv=self.inner_cv,\n",
    "                    n_jobs=1,\n",
    "                    refit=True,\n",
    "                    verbose=2         \n",
    "                )\n",
    "    \n",
    "                cv_results = cross_validate(\n",
    "                    estimator=grid_search,\n",
    "                    X=self.X,\n",
    "                    y=self.y,\n",
    "                    cv=self.outer_cv,\n",
    "                    return_train_score=True,\n",
    "                    return_estimator=True,\n",
    "                    scoring=\"f1_macro\",\n",
    "                    n_jobs=1,\n",
    "                    verbose=2\n",
    "                )\n",
    "    \n",
    "                best_fold_index = cv_results['test_score'].argmax()\n",
    "                self.process_and_store_results(model_name, cv_results, best_fold_index)\n",
    "        \n",
    "        finally:\n",
    "            self.ml_logger.results_to_csv(self.results_buffer)\n",
    "            print(\"All results saved by the logger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea79d6-b9b2-4d0a-969f-9ad9524754c6",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6034e5-2146-4ed7-9e0d-a1dd3aa0bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the base directory for the process\n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9dc7f1-29e9-4df8-971c-52fa76114df4",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "Use the cell below to load a data file."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7198b840-995d-4046-887e-67c7493e8ecb",
   "metadata": {},
   "source": [
    "# Load data here\n",
    "# Example with Iris\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target)\n",
    "\n",
    "columns = iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d9469e-7556-4f39-9f0e-976ed5226828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is ingested from the working directory, with the index set to col_0 and dtypes applied directly\n",
    "with open('../data/dtypes.json', 'r') as file:\n",
    "    train = pd.read_csv('../data/preproc_train.csv', index_col=0, dtype=json.load(file), low_memory=True)\n",
    "\n",
    "# Data is ingested from the working directory, with the index set to col_0 and dtypes applied directly\n",
    "with open('../data/dtypes.json', 'r') as file:\n",
    "    test = pd.read_csv('../data/preproc_test.csv', index_col=0, dtype=json.load(file), low_memory=True)\n",
    "\n",
    "# Convert all NaN values to np.nan\n",
    "train = train.fillna(np.nan)\n",
    "\n",
    "X = train.drop(columns=['claim_injury_type']).copy()\n",
    "y = train['claim_injury_type']\n",
    "\n",
    "# Drop missing from y\n",
    "y = y.dropna()\n",
    "\n",
    "# Align X and y based on y's indices\n",
    "X = X.loc[y.index]\n",
    "\n",
    "# Now select numeric columns for X, and sample 20k for ease of testing\n",
    "X = X.sample(n=20000)\n",
    "\n",
    "# Slice y on the selected X indices\n",
    "y = y[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25fa1141-3eeb-4c51-995a-af61acf0d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_missing_features = [\n",
    "    'accident_date',\n",
    "    'age_at_injury',\n",
    "    'average_weekly_wage',\n",
    "    'birth_year',\n",
    "    'c_2_date',\n",
    "    'c_3_date',\n",
    "    'first_hearing_date',\n",
    "    'gender',\n",
    "    'ime_4_count',\n",
    "    'industry_code',\n",
    "    'industry_code_description',\n",
    "    'wcio_cause_of_injury_code',\n",
    "    'wcio_cause_of_injury_description',\n",
    "    'wcio_nature_of_injury_code',\n",
    "    'wcio_nature_of_injury_description',\n",
    "    'wcio_part_of_body_code',\n",
    "    'wcio_part_of_body_description'\n",
    "]\n",
    "\n",
    "metric_features = [\n",
    "    'dd_asb_c2',\n",
    "    'dd_asb_c3',\n",
    "    'dd_c2_c3',\n",
    "    'age_at_injury', \n",
    "    'ime_4_count', \n",
    "    'average_weekly_wage', \n",
    "    'birth_year',\n",
    "    'dependants',\n",
    "    'first_hearing_date_day',\n",
    "    'first_hearing_date_month',\n",
    "    'first_hearing_date_year',\n",
    "    'c_2_date_day',\n",
    "    'c_2_date_month',\n",
    "    'c_2_date_year',\n",
    "    'c_3_date_day',\n",
    "    'c_3_date_month',\n",
    "    'c_3_date_year',\n",
    "    'assembly_date_day',\n",
    "    'assembly_date_month',\n",
    "    'assembly_date_year',\n",
    "    'accident_date_day',\n",
    "    'accident_date_month',\n",
    "    'accident_date_year',\n",
    "]\n",
    "\n",
    "binary_features = [\n",
    "    'age_at_injury_zero',\n",
    "    'is_unionized',\n",
    "    'alternative_dispute_resolution',\n",
    "    'attorney_representative',\n",
    "    'covid_19_indicator',\n",
    "    'do_1',\n",
    "    'do_10',\n",
    "    'do_11',\n",
    "    'do_12',\n",
    "    'do_13',\n",
    "    'do_14',\n",
    "    'do_15',\n",
    "    'do_16',\n",
    "    'do_2',\n",
    "    'do_3',\n",
    "    'do_4',\n",
    "    'do_5',\n",
    "    'do_6',\n",
    "    'do_7',\n",
    "    'do_8',\n",
    "    'do_9',\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'carrier_name',\n",
    "    'carrier_type',\n",
    "    'county_of_injury',\n",
    "    'district_name',\n",
    "    'industry_code',\n",
    "    'industry_code_description',\n",
    "    'medical_fee_region',\n",
    "    'wcio_cause_of_injury_code',\n",
    "    'wcio_cause_of_injury_description',\n",
    "    'wcio_nature_of_injury_code',\n",
    "    'wcio_nature_of_injury_description',\n",
    "    'wcio_part_of_body_code',\n",
    "    'wcio_part_of_body_description',\n",
    "    'zip_code',\n",
    "    'cause_of_injury_group'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d5d1feb-bf90-42e5-b4a5-ac0ad9969078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carrier_type',\n",
       " 'county_of_injury',\n",
       " 'medical_fee_region',\n",
       " 'cause_of_injury_group',\n",
       " 'part_of_body_group']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.select_dtypes(include='category').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea55b9f-38d9-4ad7-9439-c4e89176763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3727\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X.isna().sum().sum())  # Total count of NaNs in X\n",
    "print(y.isna().sum())  # Total count of NaNs in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b45ba7d-d479-469f-898e-966ce8c17d3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 572325 entries, 5393875 to 6165075\n",
      "Data columns (total 90 columns):\n",
      " #   Column                                     Non-Null Count   Dtype   \n",
      "---  ------                                     --------------   -----   \n",
      " 0   age_at_injury                              566814 non-null  float64 \n",
      " 1   alternative_dispute_resolution             572325 non-null  float64 \n",
      " 2   attorney_representative                    572325 non-null  float64 \n",
      " 3   average_weekly_wage                        543741 non-null  float64 \n",
      " 4   birth_year                                 543333 non-null  float64 \n",
      " 5   carrier_type                               572325 non-null  category\n",
      " 6   claim_injury_type                          572325 non-null  category\n",
      " 7   county_of_injury                           572325 non-null  category\n",
      " 8   covid_19_indicator                         572325 non-null  float64 \n",
      " 9   gender                                     567575 non-null  float64 \n",
      " 10  ime_4_count                                572325 non-null  float64 \n",
      " 11  medical_fee_region                         572325 non-null  category\n",
      " 12  number_of_dependents                       572325 non-null  float64 \n",
      " 13  missing_accident_date                      572325 non-null  float64 \n",
      " 14  missing_age_at_injury                      572325 non-null  float64 \n",
      " 15  missing_average_weekly_wage                572325 non-null  float64 \n",
      " 16  missing_birth_year                         572325 non-null  float64 \n",
      " 17  missing_c_2_date                           572325 non-null  float64 \n",
      " 18  missing_c_3_date                           572325 non-null  float64 \n",
      " 19  missing_first_hearing_date                 572325 non-null  float64 \n",
      " 20  missing_gender                             572325 non-null  float64 \n",
      " 21  missing_ime_4_count                        572325 non-null  float64 \n",
      " 22  missing_industry_code                      572325 non-null  float64 \n",
      " 23  missing_industry_code_description          572325 non-null  float64 \n",
      " 24  missing_wcio_cause_of_injury_code          572325 non-null  float64 \n",
      " 25  missing_wcio_cause_of_injury_description   572325 non-null  float64 \n",
      " 26  missing_wcio_nature_of_injury_code         572325 non-null  float64 \n",
      " 27  missing_wcio_nature_of_injury_description  572325 non-null  float64 \n",
      " 28  missing_wcio_part_of_body_code             572325 non-null  float64 \n",
      " 29  missing_wcio_part_of_body_description      572325 non-null  float64 \n",
      " 30  missing_zip_code                           572325 non-null  float64 \n",
      " 31  dd_asb_c2                                  572325 non-null  float64 \n",
      " 32  dd_asb_c3                                  572325 non-null  float64 \n",
      " 33  dd_c2_c3                                   572325 non-null  float64 \n",
      " 34  do_1                                       572325 non-null  float64 \n",
      " 35  do_2                                       572325 non-null  float64 \n",
      " 36  do_3                                       572325 non-null  float64 \n",
      " 37  do_4                                       572325 non-null  float64 \n",
      " 38  do_5                                       572325 non-null  float64 \n",
      " 39  do_6                                       572325 non-null  float64 \n",
      " 40  do_7                                       572325 non-null  float64 \n",
      " 41  do_8                                       572325 non-null  float64 \n",
      " 42  do_9                                       572325 non-null  float64 \n",
      " 43  do_10                                      572325 non-null  float64 \n",
      " 44  do_11                                      572325 non-null  float64 \n",
      " 45  do_12                                      572325 non-null  float64 \n",
      " 46  do_13                                      572325 non-null  float64 \n",
      " 47  do_14                                      572325 non-null  float64 \n",
      " 48  do_15                                      572325 non-null  float64 \n",
      " 49  do_16                                      572325 non-null  float64 \n",
      " 50  accident_date_year                         571821 non-null  float64 \n",
      " 51  accident_date_month                        568636 non-null  float64 \n",
      " 52  accident_date_day                          568636 non-null  float64 \n",
      " 53  first_hearing_date_year                    572325 non-null  float64 \n",
      " 54  first_hearing_date_month                   572325 non-null  float64 \n",
      " 55  first_hearing_date_day                     572325 non-null  float64 \n",
      " 56  assembly_date_year                         572325 non-null  float64 \n",
      " 57  assembly_date_month                        572325 non-null  float64 \n",
      " 58  assembly_date_day                          572325 non-null  float64 \n",
      " 59  c_2_date_year                              572325 non-null  float64 \n",
      " 60  c_2_date_month                             572325 non-null  float64 \n",
      " 61  c_2_date_day                               572325 non-null  float64 \n",
      " 62  c_3_date_year                              572325 non-null  float64 \n",
      " 63  c_3_date_month                             572325 non-null  float64 \n",
      " 64  c_3_date_day                               572325 non-null  float64 \n",
      " 65  age_at_injury_zero                         572325 non-null  float64 \n",
      " 66  is_unionized                               572325 non-null  float64 \n",
      " 67  cause_of_injury_group                      556702 non-null  category\n",
      " 68  part_of_body_group                         553511 non-null  category\n",
      " 69  avg_word_emb_dim_0                         572325 non-null  float64 \n",
      " 70  avg_word_emb_dim_1                         572325 non-null  float64 \n",
      " 71  avg_word_emb_dim_2                         572325 non-null  float64 \n",
      " 72  avg_word_emb_dim_3                         572325 non-null  float64 \n",
      " 73  avg_word_emb_dim_4                         572325 non-null  float64 \n",
      " 74  avg_word_emb_dim_5                         572325 non-null  float64 \n",
      " 75  avg_word_emb_dim_6                         572325 non-null  float64 \n",
      " 76  avg_word_emb_dim_7                         572325 non-null  float64 \n",
      " 77  avg_word_emb_dim_8                         572325 non-null  float64 \n",
      " 78  avg_word_emb_dim_9                         572325 non-null  float64 \n",
      " 79  var_word_emb_dim_0                         572325 non-null  float64 \n",
      " 80  var_word_emb_dim_1                         572325 non-null  float64 \n",
      " 81  var_word_emb_dim_2                         572325 non-null  float64 \n",
      " 82  var_word_emb_dim_3                         572325 non-null  float64 \n",
      " 83  var_word_emb_dim_4                         572325 non-null  float64 \n",
      " 84  var_word_emb_dim_5                         572325 non-null  float64 \n",
      " 85  var_word_emb_dim_6                         572325 non-null  float64 \n",
      " 86  var_word_emb_dim_7                         572325 non-null  float64 \n",
      " 87  var_word_emb_dim_8                         572325 non-null  float64 \n",
      " 88  var_word_emb_dim_9                         572325 non-null  float64 \n",
      " 89  euclidean_norm                             572325 non-null  float64 \n",
      "dtypes: category(6), float64(84)\n",
      "memory usage: 390.6 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb707a-e112-446b-a091-7796b9357243",
   "metadata": {},
   "source": [
    "## Generate instructions\n",
    "\n",
    "Import all necessary libraries here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "573a5a99-b9a6-4480-beca-13214711a06f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transformer imports\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Recursive Feature Selection\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Predictor imports\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7da330c-8012-46a6-9521-5e75a8ccd3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an instance of the Auditer\n",
    "auditer = MLAuditer(base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c5b5925-d64c-4a81-9b0b-b293b15a31b8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "column_names = list(X)\n",
    "mario = IterPlumber(column_names=column_names, auditer=auditer)\n",
    "\n",
    "# Run pipes\n",
    "mario.run_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f759f72-a2ca-4dbc-aed0-af9523113f0c",
   "metadata": {},
   "source": [
    "## Fit data\n",
    "\n",
    "Consumes instructions, as pipelines, via gridsearch, cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cba1a76-6da9-43c4-85f3-09de9b994884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "[('randomforestclassifier', Pipeline(steps=[('column_transformer_simpleimputer',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('simpleimputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent'),\n",
      "                                                  ['carrier_type',\n",
      "                                                   'county_of_injury',\n",
      "                                                   'part_of_body_group',\n",
      "                                                   'cause_of_injury_group'])])),\n",
      "                ('column_transformer_onehotencoder',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('onehot...\n",
      "                                                   'c_2_date_month',\n",
      "                                                   'c_2_date_day',\n",
      "                                                   'c_3_date_year',\n",
      "                                                   'c_3_date_month',\n",
      "                                                   'c_3_date_day',\n",
      "                                                   'avg_word_emb_dim_0',\n",
      "                                                   'avg_word_emb_dim_1',\n",
      "                                                   'avg_word_emb_dim_2',\n",
      "                                                   'avg_word_emb_dim_3',\n",
      "                                                   'avg_word_emb_dim_4',\n",
      "                                                   'avg_word_emb_dim_5', ...])])),\n",
      "                ('rfe',\n",
      "                 RFE(estimator=LogisticRegression(max_iter=200),\n",
      "                     n_features_to_select=50, step=2)),\n",
      "                ('randomforestclassifier', RandomForestClassifier())]), {})]\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.1s\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.1s\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.1s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No result data to save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 147\u001b[0m, in \u001b[0;36mFitter.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m    138\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipeline,\n\u001b[1;32m    139\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m         \n\u001b[1;32m    145\u001b[0m )\n\u001b[0;32m--> 147\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[1;32m    148\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mgrid_search,\n\u001b[1;32m    149\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX,\n\u001b[1;32m    150\u001b[0m     y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my,\n\u001b[1;32m    151\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mouter_cv,\n\u001b[1;32m    152\u001b[0m     return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    153\u001b[0m     return_estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    156\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m best_fold_index \u001b[38;5;241m=\u001b[39m cv_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39margmax()\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:443\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    423\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    425\u001b[0m         clone(estimator),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[1;32m    441\u001b[0m )\n\u001b[0;32m--> 443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[0;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_search.py\", line 1018, in fit\n    self._run_search(evaluate_candidates)\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_search.py\", line 1572, in _run_search\n    evaluate_candidates(ParameterGrid(self.param_grid))\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_search.py\", line 995, in evaluate_candidates\n    _warn_or_raise_about_fit_failures(out, self.error_score)\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 529, in _warn_or_raise_about_fit_failures\n    raise ValueError(all_fits_failed_message)\nValueError: \nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'carrier_type'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/_indexing.py\", line 361, in _get_column_indices\n    col_idx = all_columns.get_loc(col)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'carrier_type'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/imblearn/pipeline.py\", line 329, in fit\n    Xt, yt = self._fit(X, y, routed_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/imblearn/pipeline.py\", line 255, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/imblearn/pipeline.py\", line 1104, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 313, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py\", line 968, in fit_transform\n    self._validate_column_callables(X)\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py\", line 536, in _validate_column_callables\n    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/_indexing.py\", line 369, in _get_column_indices\n    raise ValueError(\"A given column is not a column of the dataframe\") from e\nValueError: A given column is not a column of the dataframe\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m fitter \u001b[38;5;241m=\u001b[39m Fitter(X_train, y_train, instructions_json\u001b[38;5;241m=\u001b[39minstructions, logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Run the fitting process\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m fitter\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[0;32mIn[5], line 163\u001b[0m, in \u001b[0;36mFitter.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_and_store_results(model_name, cv_results, best_fold_index)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mml_logger\u001b[38;5;241m.\u001b[39mresults_to_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_buffer)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll results saved by the logger.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 139\u001b[0m, in \u001b[0;36mMLLogger.results_to_csv\u001b[0;34m(self, result_data)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mAppends results to a CSV file, injecting the config_id and run_id.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result_data:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo result data to save.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(result_data)\n\u001b[1;32m    142\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id\n",
      "\u001b[0;31mValueError\u001b[0m: No result data to save."
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Auditer passes a valid instruction set \n",
    "instructions = auditer.get_target_dir_target_id(target_dir=\"config\", target_id=\"2\")\n",
    "\n",
    "# Creates an instance of the Logger\n",
    "logger = MLLogger(base_dir=base_dir, instructions_json=instructions)\n",
    "\n",
    "# Create the Fitter instance\n",
    "fitter = Fitter(X_train, y_train, instructions_json=instructions, logger=logger)\n",
    "\n",
    "# Run the fitting process\n",
    "fitter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9578c8-d54d-4e41-a651-471edccfc859",
   "metadata": {},
   "source": [
    "Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811bb6e2-1180-425a-b032-e8dd0e52c60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
