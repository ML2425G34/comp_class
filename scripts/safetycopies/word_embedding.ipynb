{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "This file is used to create versions of preprocessment, for the modelling stage. It includes the treatment of missing values, inconsistent values, as well as feature engineering, both elementary and advanced. It does not include, scaling, outlier removal or feature selection of any kind.\n",
    "\n",
    "Each transformation is performed both on train, as well as test data - wherever applicable. This script was written with the assistance of artificial tools, to generate boilerplates, and correct grammar; the authors attest to its correctness with penalty of grade.\n",
    "\n",
    "DISCLAIMER: The steps taken in this notebook to treat the data do not make a superlative effort to justify or demonstrate themselves. Look instead to the Data Exploration notebook, that goes into lengthy detail over the reasoning behind each preprocessment choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Libraries used in the elaboration of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL\n",
    "import os \n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Ingestion and Manipulation\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# fastext\n",
    "import fasttext \n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Assist in the inspection and execution of preprocessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to_snake_case\n",
    "\n",
    "Converts a string to snake_case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(input_string):\n",
    "    \"\"\"\n",
    "    Converts a given string to snake_case.\n",
    "\n",
    "    Args:\n",
    "        input_string (str): The string to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: The snake_case version of the input string.\n",
    "    \"\"\"\n",
    "    # Replace spaces or hyphens with underscores\n",
    "    input_string = re.sub(r\"[\\s\\-\\/]+\", \"_\", input_string)\n",
    "    \n",
    "    # Add underscores between camelCase or PascalCase words\n",
    "    input_string = re.sub(r\"([a-z])([A-Z])\", r\"\\1_\\2\", input_string)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    snake_case_string = input_string.lower()\n",
    "    \n",
    "    return snake_case_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Data was loaded and our prefered naming convention (PEP-8 - snake_case) was adopted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest at the working directory\n",
    "train = pd.read_csv('../data/train_data.csv', low_memory=False)\n",
    "\n",
    "# Convert variables to snake case\n",
    "train.rename(columns={colname : to_snake_case(colname) for colname in train.columns}, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest at working directory\n",
    "test = pd.read_csv('../data/test_data.csv', low_memory=False)\n",
    "\n",
    "# Convert variables to snake case\n",
    "test.rename(columns={colname : to_snake_case(colname) for colname in test.columns}, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindexing the dataframe with Claim Identifier\n",
    "\n",
    "Converted to index, as it is the natural index for the data, to this end, both duplicates and inconsistent values were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "train.drop_duplicates(subset=['claim_identifier'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping: 19444 rows\n"
     ]
    }
   ],
   "source": [
    "# Boolean mask to select the values of interest\n",
    "id_mask_df = train[train['claim_identifier'].astype(str).str.len() != 7]\n",
    "train = train.drop(id_mask_df.index)\n",
    "\n",
    "print(f'Droping: {id_mask_df.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim_identifier is set as index to train dataframe\n",
    "train = train.set_index(keys='claim_identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim_identifier is set as index to test dataframe\n",
    "test = test.set_index(keys='claim_identifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping useless columns\n",
    "\n",
    "Columns which it was immediately evident will serve no purpose were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not present in test, empty column, unary\n",
    "train = train.drop(columns=['agreement_reached', 'oiics_nature_of_injury_description', 'wcb_decision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty column, unary \n",
    "test = test.drop(columns=['oiics_nature_of_injury_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also cast the feature itself to a category\n",
    "train['claim_injury_type'] = train['claim_injury_type'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 574026 entries, 5393875 to 6165075\n",
      "Data columns (total 29 columns):\n",
      " #   Column                             Non-Null Count   Dtype   \n",
      "---  ------                             --------------   -----   \n",
      " 0   accident_date                      570337 non-null  object  \n",
      " 1   age_at_injury                      574026 non-null  float64 \n",
      " 2   alternative_dispute_resolution     574026 non-null  object  \n",
      " 3   assembly_date                      574026 non-null  object  \n",
      " 4   attorney_representative            574026 non-null  object  \n",
      " 5   average_weekly_wage                545375 non-null  float64 \n",
      " 6   birth_year                         544948 non-null  float64 \n",
      " 7   c_2_date                           559466 non-null  object  \n",
      " 8   c_3_date                           187245 non-null  object  \n",
      " 9   carrier_name                       574026 non-null  object  \n",
      " 10  carrier_type                       574026 non-null  object  \n",
      " 11  claim_injury_type                  574026 non-null  category\n",
      " 12  county_of_injury                   574026 non-null  object  \n",
      " 13  covid_19_indicator                 574026 non-null  object  \n",
      " 14  district_name                      574026 non-null  object  \n",
      " 15  first_hearing_date                 150798 non-null  object  \n",
      " 16  gender                             574026 non-null  object  \n",
      " 17  ime_4_count                        132803 non-null  float64 \n",
      " 18  industry_code                      564068 non-null  float64 \n",
      " 19  industry_code_description          564068 non-null  object  \n",
      " 20  medical_fee_region                 574026 non-null  object  \n",
      " 21  wcio_cause_of_injury_code          558386 non-null  float64 \n",
      " 22  wcio_cause_of_injury_description   558386 non-null  object  \n",
      " 23  wcio_nature_of_injury_code         558369 non-null  float64 \n",
      " 24  wcio_nature_of_injury_description  558369 non-null  object  \n",
      " 25  wcio_part_of_body_code             556944 non-null  float64 \n",
      " 26  wcio_part_of_body_description      556944 non-null  object  \n",
      " 27  zip_code                           545389 non-null  object  \n",
      " 28  number_of_dependents               574026 non-null  float64 \n",
      "dtypes: category(1), float64(9), object(19)\n",
      "memory usage: 127.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordEmbedding\n",
    "\n",
    "Using Fasttext and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTING\n",
    "\n",
    "Trying to use smote to create a more balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We first build our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Preprocessing Functions\n",
    "def remove_numbers_punctuation(text):\n",
    "    \"\"\"Remove numbers and punctuation from the text.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z\\s_]', '', text)\n",
    "\n",
    "def to_lowercase(text):\n",
    "    \"\"\"Convert text to lowercase.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def to_snake_case(text):\n",
    "    \"\"\"Convert text to snake_case.\"\"\"\n",
    "    words = text.split()\n",
    "    return '_'.join(words).lstrip('_')\n",
    "\n",
    "# 2. FastText Embedding Functions\n",
    "def extract_embeddings(sentence, model):\n",
    "    \"\"\"Extract the word embeddings for a given sentence.\"\"\"\n",
    "    words = sentence.split()  # Tokenize sentence into words\n",
    "    word_vectors = []\n",
    "\n",
    "    # Get word embeddings for each word\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_vector = model.get_word_vector(word)  # Get word vector from FastText model\n",
    "            word_vectors.append(word_vector)\n",
    "        except KeyError:\n",
    "            continue  # Skip words not in the model's vocabulary\n",
    "    \n",
    "    # If no valid word vectors, return None\n",
    "    if not word_vectors:\n",
    "        return None\n",
    "    return np.array(word_vectors)\n",
    "\n",
    "def compute_embedding_features(word_vectors):\n",
    "    \"\"\"Compute the average embedding, variance, and Euclidean norm from word vectors.\"\"\"\n",
    "    if word_vectors is None or len(word_vectors) == 0:\n",
    "        return np.zeros(300), np.zeros(300), 0  # Assuming 300 dimensions for FastText embeddings\n",
    "    \n",
    "    # Compute the average embedding (mean across words)\n",
    "    average_embedding = np.mean(word_vectors, axis=0)\n",
    "    \n",
    "    # Compute the variance per dimension\n",
    "    variance_embedding = np.var(word_vectors, axis=0)\n",
    "    \n",
    "    # Compute the Euclidean norm of the average embedding\n",
    "    euclidean_norm = np.linalg.norm(average_embedding)\n",
    "    \n",
    "    return average_embedding, variance_embedding, euclidean_norm\n",
    "\n",
    "# 3. Main Processing Pipeline\n",
    "def process_data(train, columns_to_embed, model_path=r'../models/dbpedia.bin'):\n",
    "    \"\"\"Preprocess and extract features for the input data using a pretrained FastText model.\"\"\"\n",
    "    # Load your FastText model (pretrained, e.g., dbpedia or other embeddings)\n",
    "    model = fasttext.load_model(model_path)\n",
    "    \n",
    "    # Select the features from the training data\n",
    "    data = train[columns_to_embed].copy()\n",
    "\n",
    "    # Preserve the original 'claim_identifier' from the train index\n",
    "    data['claim_identifier'] = train.index  # Using train.index as the 'claim_identifier'\n",
    "\n",
    "    # Apply text transformations only to non-missing rows\n",
    "    for col in data.columns:\n",
    "        if col != 'claim_identifier':  # Don't transform the 'claim_identifier' column\n",
    "            for idx, value in data[col].items():\n",
    "                try:\n",
    "                    if pd.notna(value):  # Check if the value is not NaN\n",
    "                        value = remove_numbers_punctuation(value)\n",
    "                        value = to_lowercase(value)\n",
    "                        data.at[idx, col] = value  # Update the value in the DataFrame\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing value '{value}' in column '{col}': {e}\")\n",
    "                    data.at[idx, col] = np.nan  # If error occurs, set value to NaN\n",
    "                    \n",
    "    # Concatenate features to prepare for FastText input\n",
    "    data['text_features'] = data.apply(\n",
    "        lambda row: ' '.join(str(val) for val in row if val != 'claim_identifier' and pd.notna(val)), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Extract word embeddings row-wise\n",
    "    def compute_features_for_row(row):\n",
    "        try:\n",
    "            word_vectors = extract_embeddings(row['text_features'], model)\n",
    "            return compute_embedding_features(word_vectors)\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing features for row: {e}\")\n",
    "            dimension = model.get_dimension()\n",
    "            return (np.zeros(dimension), np.zeros(dimension), 0)\n",
    "\n",
    "    # Apply row-wise extraction of embeddings and features\n",
    "    data['embedding_features'] = data.apply(compute_features_for_row, axis=1)\n",
    "    \n",
    "    # Separate average embedding dimensions into individual columns\n",
    "    avg_dim_columns = [f'avg_word_emb_dim_{i}' for i in range(model.get_dimension())]\n",
    "    data[avg_dim_columns] = pd.DataFrame(data['embedding_features'].map(lambda x: x[0]).tolist(), index=data.index)\n",
    "    \n",
    "    # Separate variance embedding dimensions into individual columns\n",
    "    var_dim_columns = [f'var_word_emb_dim_{i}' for i in range(model.get_dimension())]\n",
    "    data[var_dim_columns] = pd.DataFrame(data['embedding_features'].map(lambda x: x[1]).tolist(), index=data.index)\n",
    "    \n",
    "    # Add Euclidean norm as a single column\n",
    "    data['euclidean_norm'] = data['embedding_features'].map(lambda x: x[2])\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    data = data.drop(columns=['text_features', 'embedding_features', 'claim_identifier'] + columns_to_embed)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 4. Deploy\n",
    "columns_to_embed = [\n",
    "    'wcio_cause_of_injury_description',\n",
    "    'wcio_part_of_body_description',\n",
    "    'wcio_nature_of_injury_description',\n",
    "    'industry_code_description',\n",
    "    'carrier_type'\n",
    "]\n",
    "\n",
    "# Process the data and extract the required features\n",
    "processed_data = process_data(train, columns_to_embed)\n",
    "\n",
    "# Concatenate the original `train` DataFrame with `processed_data`\n",
    "train = pd.concat([train, processed_data], axis=1)\n",
    "train.drop(columns=columns_to_embed, inplace=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
