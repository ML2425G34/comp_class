{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d41b10-0c77-40e2-ac8c-44ebdfca6114",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca557089-8255-4d01-b414-4dcb3bbdfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import inspect\n",
    "from collections.abc import Iterable\n",
    "\n",
    "# Base Classes & Estimators\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Pipeline & Model Construction\n",
    "from imblearn.pipeline import Pipeline as imPipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocessing & Transformation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Model Selection & Estimators\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Model Evaluation & Scoring\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer\n",
    "\n",
    "# Handling Imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Model Tuning & Cross-validation\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237db99-4514-4227-a012-f519349cd5b9",
   "metadata": {},
   "source": [
    "# Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c2b034-caae-4917-8f6c-81813eb6e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierClipper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_percentile=0.005, upper_percentile=0.995, use_iqr=False):\n",
    "        \"\"\"\n",
    "        Initialize the OutlierClipper with options for percentile clipping or IQR-based clipping.\n",
    "\n",
    "        Parameters:\n",
    "        - lower_percentile: float, lower bound percentile for clipping (if percentiles are used)\n",
    "        - upper_percentile: float, upper bound percentile for clipping (if percentiles are used)\n",
    "        - use_iqr: bool, whether to use IQR method for determining bounds\n",
    "        \"\"\"\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "        self.use_iqr = use_iqr\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the clipping bounds based on the training dataset using the specified method (percentiles or IQR).\n",
    "\n",
    "        Parameters:\n",
    "        - X: numpy.ndarray or pandas.DataFrame, the dataset used for fitting\n",
    "        - y: ignored, not used for fitting\n",
    "\n",
    "        Returns:\n",
    "        - self: fitted instance of the class\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if input is numpy array\n",
    "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "        \n",
    "        # For each column in X, calculate the bounds using the specified method\n",
    "        self.bounds_ = {}\n",
    "        for column in X.columns:\n",
    "            if self.use_iqr:\n",
    "                q1 = X[column].quantile(0.25)  # 1st quartile\n",
    "                q3 = X[column].quantile(0.75)  # 3rd quartile\n",
    "                iqr = q3 - q1  # Interquartile range\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "            else:\n",
    "                lower_bound = X[column].quantile(self.lower_percentile)\n",
    "                upper_bound = X[column].quantile(self.upper_percentile)\n",
    "\n",
    "            self.bounds_[column] = (lower_bound, upper_bound)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply clipping to the dataset based on the fitted bounds.\n",
    "\n",
    "        Parameters:\n",
    "        - X: numpy.ndarray or pandas.DataFrame, the dataset to transform\n",
    "\n",
    "        Returns:\n",
    "        - X: pandas.DataFrame, the transformed dataset with clipped values\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if input is numpy array\n",
    "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "        # Apply clipping for each column\n",
    "        for column, (lower_bound, upper_bound) in self.bounds_.items():\n",
    "            X[column] = X[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def set_output(self, transform=\"default\"):\n",
    "        \"\"\"\n",
    "        Enable compatibility with scikit-learn's `set_output` functionality.\n",
    "\n",
    "        Parameters:\n",
    "        - transform: str, the output format (\"default\" or \"pandas\").\n",
    "        \"\"\"\n",
    "        self.output_format = transform\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de57347-467e-4d23-9175-fe9803c7f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Compute the frequency counts for each column in the DataFrame\n",
    "        self.freq_map = X.apply(pd.Series.value_counts)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Apply the frequency counts to transform the data\n",
    "        return X.apply(lambda col: col.map(self.freq_map[col.name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9999c-088c-40b1-8b6f-5d1304df3c30",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b3b112-689d-4a91-bb32-589dc38acbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is ingested from the working directory, with the index set to col_0 and dtypes applied directly\n",
    "with open('../data/dtypes.json', 'r') as file:\n",
    "    train = pd.read_csv('../data/preproc_train.csv', index_col=0, dtype=json.load(file), low_memory=True)\n",
    "\n",
    "# Data is ingested from the working directory, with the index set to col_0 and dtypes applied directly\n",
    "with open('../data/dtypes.json', 'r') as file:\n",
    "    test = pd.read_csv('../data/preproc_test.csv', index_col=0, dtype=json.load(file), low_memory=True)\n",
    "\n",
    "# Convert all NaN values to np.nan\n",
    "train = train.fillna(np.nan)\n",
    "\n",
    "# Separate features (X) and target\n",
    "X = train.drop(columns=['claim_injury_type']).copy()\n",
    "target = train['claim_injury_type']\n",
    "\n",
    "# Map y to the first character of target and drop missing values\n",
    "y = target.map(lambda target: target[0]).astype(int)\n",
    "y = y.dropna()\n",
    "\n",
    "# Align X and y based on y's indices\n",
    "X = X.loc[y.index]\n",
    "\n",
    "# Ensure all rows where y is in [6, 7, 8] are included\n",
    "rows_with_78 = y[y.isin([7, 8])].index\n",
    "\n",
    "# Extract these rows\n",
    "X_with_78 = X.loc[rows_with_78]\n",
    "y_with_78 = y.loc[rows_with_78]\n",
    "\n",
    "# Calculate the remaining sample size\n",
    "remaining_sample_size = 10000 - len(X_with_78)\n",
    "\n",
    "# Exclude rows with y values [6, 7, 8] from the rest of the dataset\n",
    "X_remaining = X.drop(index=rows_with_78)\n",
    "y_remaining = y.drop(index=rows_with_78)\n",
    "\n",
    "# Sample the required number of rows from the remaining dataset\n",
    "X_sampled = X_remaining.sample(remaining_sample_size, random_state=42)\n",
    "y_sampled = y_remaining.loc[X_sampled.index]\n",
    "\n",
    "# Combine the rows for y values [6, 7, 8] with the sampled rows\n",
    "X = pd.concat([X_with_78, X_sampled])\n",
    "y = pd.concat([y_with_78, y_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf420dce-7ed3-435e-9747-fa9cd8f1c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features = [\n",
    "    'age_at_injury', \n",
    "    'ime_4_count', \n",
    "    'average_weekly_wage', \n",
    "    'birth_year',\n",
    "    'number_of_dependents',\n",
    "    'dd_asb_c2',\n",
    "    'dd_asb_c3',\n",
    "    'dd_c2_c3',\n",
    "    'first_hearing_date_day',\n",
    "    'first_hearing_date_month',\n",
    "    'first_hearing_date_year',\n",
    "    'c_2_date_day',\n",
    "    'c_2_date_month',\n",
    "    'c_2_date_year',\n",
    "    'c_3_date_day',\n",
    "    'c_3_date_month',\n",
    "    'c_3_date_year',\n",
    "    'assembly_date_day',\n",
    "    'assembly_date_month',\n",
    "    'assembly_date_year',\n",
    "    'accident_date_day',\n",
    "    'accident_date_month',\n",
    "    'accident_date_year',\n",
    "    'avg_word_emb_dim_0',\n",
    "    'avg_word_emb_dim_1',\n",
    "    'avg_word_emb_dim_2',\n",
    "    'avg_word_emb_dim_3',\n",
    "    'avg_word_emb_dim_4',\n",
    "    'avg_word_emb_dim_5',\n",
    "    'avg_word_emb_dim_6',\n",
    "    'avg_word_emb_dim_7',\n",
    "    'avg_word_emb_dim_8',\n",
    "    'avg_word_emb_dim_9',\n",
    "    'var_word_emb_dim_0',\n",
    "    'var_word_emb_dim_1',\n",
    "    'var_word_emb_dim_2',\n",
    "    'var_word_emb_dim_3',\n",
    "    'var_word_emb_dim_4',\n",
    "    'var_word_emb_dim_5',\n",
    "    'var_word_emb_dim_6',\n",
    "    'var_word_emb_dim_7',\n",
    "    'var_word_emb_dim_8',\n",
    "    'var_word_emb_dim_9',\n",
    "    'euclidean_norm'\n",
    "]\n",
    "\n",
    "binary_features = [\n",
    "    'age_at_injury_zero',\n",
    "    'is_unionized',\n",
    "    'alternative_dispute_resolution',\n",
    "    'attorney_representative',\n",
    "    'covid_19_indicator',\n",
    "    'do_1',\n",
    "    'do_10',\n",
    "    'do_11',\n",
    "    'do_12',\n",
    "    'do_13',\n",
    "    'do_14',\n",
    "    'do_15',\n",
    "    'do_16',\n",
    "    'do_2',\n",
    "    'do_3',\n",
    "    'do_4',\n",
    "    'do_5',\n",
    "    'do_6',\n",
    "    'do_7',\n",
    "    'do_8',\n",
    "    'do_9',\n",
    "    'missing_accident_date',\n",
    "    'missing_age_at_injury',\n",
    "    'missing_average_weekly_wage',\n",
    "    'missing_birth_year',\n",
    "    'missing_c_2_date',\n",
    "    'missing_c_3_date',\n",
    "    'missing_first_hearing_date',\n",
    "    'missing_gender',\n",
    "    'missing_ime_4_count',\n",
    "    'missing_industry_code',\n",
    "    'missing_industry_code_description',\n",
    "    'missing_wcio_cause_of_injury_code',\n",
    "    'missing_wcio_cause_of_injury_description',\n",
    "    'missing_wcio_nature_of_injury_code',\n",
    "    'missing_wcio_nature_of_injury_description',\n",
    "    'missing_wcio_part_of_body_code',\n",
    "    'missing_wcio_part_of_body_description',\n",
    "    'missing_zip_code'\n",
    "]\n",
    "\n",
    "hot_columns = [\n",
    "    \"carrier_type\", \n",
    "    \"part_of_body_group\", \n",
    "    \"cause_of_injury_group\",\n",
    "    \"medical_fee_region\",\n",
    "]\n",
    "frequency_columns = [\n",
    "    \"industry_code\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93523a47-6e62-4f82-88c7-ae83ac64b216",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb78696-62b1-4e3a-98b7-f31fffbe50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = imPipeline(steps=[\n",
    "    ('column_transformer', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('onehotencoder', OneHotEncoder(), hot_columns),\n",
    "            ('frequencyencoder', FrequencyEncoder(), frequency_columns),\n",
    "            ('outlier_clipper', OutlierClipper(), metric_features)\n",
    "        ]\n",
    "    )),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('simpleimputer', SimpleImputer(strategy='median')),  # Imputation step\n",
    "    ('rfe', RFE(estimator=LogisticRegression(max_iter=200, penalty='l2',solver='newton-cholesky'), step=2)),  # Feature selection\n",
    "    ('smote', SMOTE(sampling_strategy='auto', random_state=42)),  # Oversampling after RFE\n",
    "    ('random_forest', RandomForestClassifier())  # Final model\n",
    "])\n",
    "\n",
    "model_name = 'random_forest'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "951a39a0-917a-4926-8731-e1fd5858928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    # For RandomForestClassifier (model hyperparameters)\n",
    "    'random_forest__n_estimators': [50, 75, 100],\n",
    "    'random_forest__max_depth': [10, 20],\n",
    "    'random_forest__min_samples_split': [5, 10],\n",
    "    'random_forest__min_samples_leaf': [2, 4],\n",
    "    'random_forest__max_features': ['auto'],\n",
    "    \n",
    "    # For OutlierClipper (customizable clipping range)\n",
    "    'column_transformer__outlier_clipper__lower_percentile': [0.01],\n",
    "    'column_transformer__outlier_clipper__upper_percentile': [0.99]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae714aa2-daaa-40d1-96c6-1af2c3661c76",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b07187da-e16a-4d70-b43d-a2cb4570afb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'param_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m outer_cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m inner_cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m      5\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipeline,\n\u001b[0;32m----> 6\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m      7\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     cv\u001b[38;5;241m=\u001b[39minner_cv,\n\u001b[1;32m      9\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     10\u001b[0m     refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Perform cross-validation using GridSearchCV with outer cross-validation\u001b[39;00m\n\u001b[1;32m     15\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[1;32m     16\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mgrid_search,\n\u001b[1;32m     17\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'param_grid' is not defined"
     ]
    }
   ],
   "source": [
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=inner_cv,\n",
    "    n_jobs=3,\n",
    "    refit=True,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Perform cross-validation using GridSearchCV with outer cross-validation\n",
    "cv_results = cross_validate(\n",
    "    estimator=grid_search,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=outer_cv,\n",
    "    return_train_score=True,\n",
    "    return_estimator=True,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88ffc2-7bd8-4539-9934-671c4158df7b",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc762e-fdba-430e-b843-56f373ef414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cv_results to a pandas DataFrame\n",
    "cv_results_df = pd.DataFrame(df_results)\n",
    "\n",
    "# Specify the file path (in the current working directory)\n",
    "file_path = 'cv_results.csv'\n",
    "\n",
    "# Append the DataFrame to the CSV file (if it exists) or create a new one\n",
    "cv_results_df.to_csv(file_path, mode='a', header=not pd.io.common.file_exists(file_path), index=False)\n",
    "\n",
    "print(f\"Cross-validation results saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd96040-7ec7-4571-801c-83ba9033b7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
