{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d41b10-0c77-40e2-ac8c-44ebdfca6114",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca557089-8255-4d01-b414-4dcb3bbdfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import inspect\n",
    "from collections.abc import Iterable\n",
    "\n",
    "# Base Classes & Estimators\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Pipeline & Model Construction\n",
    "from imblearn.pipeline import Pipeline as imPipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Preprocessing & Transformation\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import RFE, SelectKBest\n",
    "\n",
    "# Handling Imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model Tuning & Cross-validation\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedKFold, train_test_split\n",
    "\n",
    "# Model Evaluation & Scoring\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237db99-4514-4227-a012-f519349cd5b9",
   "metadata": {},
   "source": [
    "# Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c2b034-caae-4917-8f6c-81813eb6e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierClipper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_percentile=0.005, upper_percentile=0.995, use_iqr=False):\n",
    "        \"\"\"\n",
    "        Initialize the OutlierClipper with options for percentile clipping or IQR-based clipping.\n",
    "\n",
    "        Parameters:\n",
    "        - lower_percentile: float, lower bound percentile for clipping (if percentiles are used)\n",
    "        - upper_percentile: float, upper bound percentile for clipping (if percentiles are used)\n",
    "        - use_iqr: bool, whether to use IQR method for determining bounds\n",
    "        \"\"\"\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "        self.use_iqr = use_iqr\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the clipping bounds based on the training dataset using the specified method (percentiles or IQR).\n",
    "\n",
    "        Parameters:\n",
    "        - X: numpy.ndarray or pandas.DataFrame, the dataset used for fitting\n",
    "        - y: ignored, not used for fitting\n",
    "\n",
    "        Returns:\n",
    "        - self: fitted instance of the class\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if input is numpy array\n",
    "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "        \n",
    "        # For each column in X, calculate the bounds using the specified method\n",
    "        self.bounds_ = {}\n",
    "        for column in X.columns:\n",
    "            if self.use_iqr:\n",
    "                q1 = X[column].quantile(0.25)  # 1st quartile\n",
    "                q3 = X[column].quantile(0.75)  # 3rd quartile\n",
    "                iqr = q3 - q1  # Interquartile range\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "            else:\n",
    "                lower_bound = X[column].quantile(self.lower_percentile)\n",
    "                upper_bound = X[column].quantile(self.upper_percentile)\n",
    "\n",
    "            self.bounds_[column] = (lower_bound, upper_bound)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply clipping to the dataset based on the fitted bounds.\n",
    "\n",
    "        Parameters:\n",
    "        - X: numpy.ndarray or pandas.DataFrame, the dataset to transform\n",
    "\n",
    "        Returns:\n",
    "        - X: pandas.DataFrame, the transformed dataset with clipped values\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if input is numpy array\n",
    "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "        # Apply clipping for each column\n",
    "        for column, (lower_bound, upper_bound) in self.bounds_.items():\n",
    "            X[column] = X[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def set_output(self, transform=\"default\"):\n",
    "        \"\"\"\n",
    "        Enable compatibility with scikit-learn's `set_output` functionality.\n",
    "\n",
    "        Parameters:\n",
    "        - transform: str, the output format (\"default\" or \"pandas\").\n",
    "        \"\"\"\n",
    "        self.output_format = transform\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de57347-467e-4d23-9175-fe9803c7f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Compute the frequency counts for each column in the DataFrame\n",
    "        self.freq_map = X.apply(pd.Series.value_counts)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Apply the frequency counts to transform the data\n",
    "        return X.apply(lambda col: col.map(self.freq_map[col.name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9999c-088c-40b1-8b6f-5d1304df3c30",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b3b112-689d-4a91-bb32-589dc38acbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingestion with dtype application directly\n",
    "with open('../data/dtypes.json', 'r') as file:\n",
    "    dtypes = json.load(file)\n",
    "train = pd.read_csv('../data/preproc_train.csv', index_col=0, dtype=dtypes, low_memory=True)\n",
    "test = pd.read_csv('../data/preproc_test.csv', index_col=0, dtype=dtypes, low_memory=True)\n",
    "\n",
    "# Handle NaN and target variable creation\n",
    "train = train.fillna(np.nan)\n",
    "X = train.drop(columns=['claim_injury_type'])\n",
    "y = train['claim_injury_type'].map(lambda x: int(x[0]) - 1).astype(int)\n",
    "\n",
    "# Extract rows where y is in [6, 7, 8], then sample remaining rows to match size\n",
    "rows_with_78 = y.isin([6, 7])\n",
    "X_with_78, y_with_78 = X[rows_with_78], y[rows_with_78]\n",
    "\n",
    "# Sample from the rest of the data\n",
    "X_remaining, y_remaining = X[~rows_with_78], y[~rows_with_78]\n",
    "remaining_sample_size = 10000 - len(X_with_78)\n",
    "X_sampled, y_sampled = X_remaining.sample(remaining_sample_size, random_state=42), y_remaining.loc[X_remaining.sample(remaining_sample_size, random_state=42).index]\n",
    "\n",
    "# Concatenate the data\n",
    "X, y = pd.concat([X_with_78, X_sampled]), pd.concat([y_with_78, y_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf420dce-7ed3-435e-9747-fa9cd8f1c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features = [\n",
    "    'age_at_injury', 'ime_4_count', 'average_weekly_wage', 'birth_year', 'number_of_dependents', 'dd_asb_c2', 'dd_asb_c3', 'dd_c2_c3',\n",
    "    'first_hearing_date_day', 'first_hearing_date_month', 'first_hearing_date_year', 'c_2_date_day', 'c_2_date_month', 'c_2_date_year',\n",
    "    'c_3_date_day', 'c_3_date_month', 'c_3_date_year', 'assembly_date_day', 'assembly_date_month', 'assembly_date_year',\n",
    "    'accident_date_day', 'accident_date_month', 'accident_date_year', 'avg_word_emb_dim_0', 'avg_word_emb_dim_1', 'avg_word_emb_dim_2',\n",
    "    'avg_word_emb_dim_3', 'avg_word_emb_dim_4', 'avg_word_emb_dim_5', 'avg_word_emb_dim_6', 'avg_word_emb_dim_7', 'avg_word_emb_dim_8',\n",
    "    'avg_word_emb_dim_9', 'var_word_emb_dim_0', 'var_word_emb_dim_1', 'var_word_emb_dim_2', 'var_word_emb_dim_3', 'var_word_emb_dim_4',\n",
    "    'var_word_emb_dim_5', 'var_word_emb_dim_6', 'var_word_emb_dim_7', 'var_word_emb_dim_8', 'var_word_emb_dim_9', 'euclidean_norm'\n",
    "]\n",
    "\n",
    "binary_features = [\n",
    "    'age_at_injury_zero', 'is_unionized', 'alternative_dispute_resolution', 'attorney_representative', 'covid_19_indicator', 'do_1', 'do_10',\n",
    "    'do_11', 'do_12', 'do_13', 'do_14', 'do_15', 'do_16', 'do_2', 'do_3', 'do_4', 'do_5', 'do_6', 'do_7', 'do_8', 'do_9', 'missing_accident_date',\n",
    "    'missing_age_at_injury', 'missing_average_weekly_wage', 'missing_birth_year', 'missing_c_2_date', 'missing_c_3_date', 'missing_first_hearing_date',\n",
    "    'missing_gender', 'missing_ime_4_count', 'missing_industry_code', 'missing_industry_code_description', 'missing_wcio_cause_of_injury_code',\n",
    "    'missing_wcio_cause_of_injury_description', 'missing_wcio_nature_of_injury_code', 'missing_wcio_nature_of_injury_description',\n",
    "    'missing_wcio_part_of_body_code', 'missing_wcio_part_of_body_description', 'missing_zip_code'\n",
    "]\n",
    "\n",
    "hot_columns = [\"carrier_type\", \"part_of_body_group\", \"cause_of_injury_group\", \"medical_fee_region\"]\n",
    "frequency_columns = [\"industry_code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93523a47-6e62-4f82-88c7-ae83ac64b216",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb78696-62b1-4e3a-98b7-f31fffbe50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = imPipeline(steps=[\n",
    "    ('column_transformer', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('onehotencoder', OneHotEncoder(), hot_columns),  # OneHotEncoder for categorical variables in 'hot_columns'\n",
    "            ('frequencyencoder', FrequencyEncoder(), frequency_columns),  # Frequency encoding for categorical variables in 'frequency_columns'\n",
    "            ('outlier_clipper', OutlierClipper(), metric_features)  # Outlier clipping for 'metric_features'\n",
    "        ]\n",
    "    )),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('simpleimputer', SimpleImputer(strategy='median')),  # Imputation step\n",
    "    ('rfe', RFE(estimator=LogisticRegression(max_iter=200, penalty='l2',solver='newton-cholesky'), step=2)),  # Feature selection\n",
    "    ('smote', SMOTE(sampling_strategy='auto', random_state=42)),  # Oversampling after RFE\n",
    "    ('random_forest', RandomForestClassifier())  # Final model\n",
    "])\n",
    "\n",
    "model_name = 'random_forest'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "951a39a0-917a-4926-8731-e1fd5858928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    # For OutlierClipper (customizable clipping range)\n",
    "    'column_transformer__outlier_clipper__lower_percentile': [0.01],  # Lower bound percentile for clipping outliers\n",
    "    'column_transformer__outlier_clipper__upper_percentile': [0.99],  # Upper bound percentile for clipping outliers\n",
    "    \n",
    "    # For RandomForestClassifier (model hyperparameters)\n",
    "    'random_forest__n_estimators': [50, 75, 100],  # Number of trees in the random forest\n",
    "    'random_forest__max_depth': [10, 20],  # Maximum depth of the trees to prevent overfitting\n",
    "    'random_forest__min_samples_split': [5, 10],  # Minimum number of samples required to split a node\n",
    "    'random_forest__min_samples_leaf': [2, 4]  # Minimum number of samples required at a leaf node\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae714aa2-daaa-40d1-96c6-1af2c3661c76",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b07187da-e16a-4d70-b43d-a2cb4570afb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'param_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m outer_cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m inner_cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m      5\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipeline,\n\u001b[0;32m----> 6\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m      7\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     cv\u001b[38;5;241m=\u001b[39minner_cv,\n\u001b[1;32m      9\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     10\u001b[0m     refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Perform cross-validation using GridSearchCV with outer cross-validation\u001b[39;00m\n\u001b[1;32m     15\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[1;32m     16\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mgrid_search,\n\u001b[1;32m     17\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'param_grid' is not defined"
     ]
    }
   ],
   "source": [
    "outer_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=inner_cv,\n",
    "    n_jobs=3,\n",
    "    refit=True,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Perform cross-validation using GridSearchCV with outer cross-validation\n",
    "cv_results = cross_validate(\n",
    "    estimator=grid_search,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=outer_cv,\n",
    "    return_train_score=True,\n",
    "    return_estimator=True,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88ffc2-7bd8-4539-9934-671c4158df7b",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc762e-fdba-430e-b843-56f373ef414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cv_results to a pandas DataFrame\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Specify the file path (in the current working directory)\n",
    "file_path = '../results/cv_results.csv'\n",
    "\n",
    "# Append the DataFrame to the CSV file (if it exists) or create a new one\n",
    "cv_results_df.to_csv(file_path, mode='a', header=not pd.io.common.file_exists(file_path), index=False)\n",
    "\n",
    "print(f\"Cross-validation results saved to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
