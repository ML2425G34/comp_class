{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c97169-c047-46b4-a6dd-e25b64608b53",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Start here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7712f7-c892-487e-8386-6188fc0245d6",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "Libraries used in the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c862d959-440d-4664-8873-7467b093efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import inspect\n",
    "from collections.abc import Iterable\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.ensemble import BaseEnsemble\n",
    "\n",
    "# General libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pipelines\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedKFold\n",
    "\n",
    "# Model Selection \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model assessment\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19ea44-fad9-4d4a-8483-821d15698700",
   "metadata": {},
   "source": [
    "# MLAnalytics\n",
    "\n",
    "Responsible for validating instructions, and writting results files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0124cfd1-d6e0-45fd-b700-806dbe710419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLAnalytics:\n",
    "    \"\"\"\n",
    "    A base class to handle shared functionality for analytics-related tasks\n",
    "    like logging and signing pipeline configurations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_dir):\n",
    "        \"\"\"\n",
    "        Initializes common functionality for both signing and logging modes.\n",
    "\n",
    "        Args:\n",
    "            base_dir (str): The base directory that contains subdirectories for logs and results.\n",
    "        \"\"\"\n",
    "        self.base_dir = base_dir\n",
    "        self.config_log_dir = os.path.join(self.base_dir, \"config\")\n",
    "        self.csv_file_path = os.path.join(self.base_dir, \"results\", \"best_runs.csv\")\n",
    "\n",
    "        # Ensure necessary directories exist\n",
    "        self.check_dir_exists(self.config_log_dir, create=True)\n",
    "        self.check_dir_exists(os.path.dirname(self.csv_file_path), create=True)\n",
    "\n",
    "    def check_dir_exists(self, dir_path, create=False):\n",
    "        \"\"\"Ensures that the provided directory path exists. Optionally creates it if necessary based on user input.\"\"\"\n",
    "        if dir_path and not os.path.exists(dir_path):\n",
    "            if create:\n",
    "                response = input(f\"Directory '{dir_path}' does not exist. Do you want to create it? (y/n): \").strip().lower()\n",
    "                if response == 'y':\n",
    "                    # Case when it is okay to create directory, and choice is yes.\n",
    "                    os.makedirs(dir_path, exist_ok=True)\n",
    "                    print(f\"Directory '{dir_path}' has been created.\")\n",
    "                else:\n",
    "                    # Case when it is okay to create directory, but choice is no.\n",
    "                    print(f\"Directory '{dir_path}' was not created.\")\n",
    "                    return False\n",
    "            else:\n",
    "                # Case when it is not allowed to create a directory and none valid was given.\n",
    "                raise FileNotFoundError(f\"No directory found at {dir_path}\")\n",
    "        # Case when existing directory is provided.\n",
    "        return True\n",
    "\n",
    "    def get_target_dir_current_id(self, target_dir):\n",
    "        \"\"\"\n",
    "        Returns the current available run ID by checking the run log directory.\n",
    "\n",
    "        Returns:\n",
    "            int: The current available run ID.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.check_dir_exists(target_dir, create=True):\n",
    "        \n",
    "            existing_files = os.listdir(target_dir)\n",
    "            target_ids = []\n",
    "            for f in existing_files:\n",
    "                if f.endswith('.json'):\n",
    "                    try:\n",
    "                        target_id = int(f.split('_')[-1].replace('.json', ''))\n",
    "                        target_ids.append(target_id)\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping file with unexpected name format: {f}\")\n",
    "\n",
    "            return max(target_ids, default=0) + 1\n",
    "\n",
    "    def get_target_dir_target_id(self, target_dir=None, target_id=None):\n",
    "        if not target_dir and target_id:\n",
    "            raise ValueError(\"Must provide directory and id, to fetch.\")\n",
    "\n",
    "        filename = f\"{target_dir}_id_{target_id}\"\n",
    "        \n",
    "        target_dir = os.path.join(self.base_dir, target_dir)\n",
    "\n",
    "        self.check_dir_exists(target_dir, create=False)\n",
    "        \n",
    "        try:\n",
    "            # Construct the filename\n",
    "            filename = f\"{target_dir}/{filename}.json\"\n",
    "            with open(filename, \"r\") as f:\n",
    "                instructions_json = json.load(f)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"No configuration file found at {filename}\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(f\"Failed to decode JSON in the file at {filename}\")\n",
    "\n",
    "        return instructions_json\n",
    "\n",
    "class MLAuditer(MLAnalytics):\n",
    "    \"\"\"Subclass of MLAnalytics for signing and storing pipeline configurations.\"\"\"\n",
    "\n",
    "    def sign_and_save_config(self, pipeline_config):\n",
    "        \"\"\"\n",
    "        Signs a pipeline configuration with a run_id and saves it as JSON.\n",
    "        \n",
    "        Args:\n",
    "            pipeline_config (dict): The pipeline configuration to sign and save.\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to the saved JSON file.\n",
    "        \"\"\"\n",
    "        signed_config = pipeline_config.copy()\n",
    "        signed_config[\"config_id\"] = self.get_target_dir_current_id(self.config_log_dir)  # Generate a config ID here\n",
    "\n",
    "        json_file = os.path.join(self.config_log_dir, f\"config_id_{signed_config['config_id']}.json\")\n",
    "        try:\n",
    "            with open(json_file, \"w\") as f:\n",
    "                json.dump(signed_config, f, indent=4)\n",
    "            print(f\"Signed pipeline configuration saved at: {json_file}\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving configuration: {e}\")\n",
    "            raise\n",
    "\n",
    "        return json_file\n",
    "\n",
    "\n",
    "class MLLogger(MLAnalytics):\n",
    "    \"\"\"Subclass of MLAnalytics for logging and storing results.\"\"\"\n",
    "\n",
    "    def __init__(self, base_dir, instructions_json):\n",
    "        \"\"\"\n",
    "        Initializes the logger with directory and instructions.\n",
    "        \n",
    "        Args:\n",
    "            base_dir (str): Base directory for saving results.\n",
    "            instructions_json (dict): The instructions containing configuration details.\n",
    "        \"\"\"\n",
    "        super().__init__(base_dir)\n",
    "        self.config_id = instructions_json.get(\"config_id\")  # Retrieve config_id from instructions\n",
    "\n",
    "    def results_to_csv(self, result_data):\n",
    "        \"\"\"\n",
    "        Appends results to a CSV file, injecting the config_id and run_id.\n",
    "        \n",
    "        Args:\n",
    "            result_data (list of dicts): The result data to append to the CSV.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if not result_data:\n",
    "            raise ValueError(\"No result data to save.\")\n",
    "\n",
    "        df = pd.DataFrame(result_data)\n",
    "        df['config_id'] = self.config_id\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(self.csv_file_path):\n",
    "                df.to_csv(self.csv_file_path, index=False)\n",
    "            else:\n",
    "                df.to_csv(self.csv_file_path, mode=\"a\", header=False, index=False)\n",
    "            print(f\"Results saved to {self.csv_file_path}\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving results to CSV: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b7810-d9dc-4931-a1f5-72fd967a44b8",
   "metadata": {},
   "source": [
    "# PipelineBuilder\n",
    "\n",
    "Pipelinebuilder assists with the creation of pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1de9d51c-b5a6-44df-9545-37bb289d0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineBuilder:\n",
    "    \"\"\"\n",
    "    Base class for building machine learning pipelines.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_names=None, auditer=None):\n",
    "        if self.__class__ is PipelineBuilder:\n",
    "            raise TypeError(\"Cannot instantiate abstract class PipelineBuilder.\")\n",
    "\n",
    "        self.column_names = column_names or []  # Store column names if provided\n",
    "        self._blueprints = {}  # Dictionary to hold pipeline instructions\n",
    "                \n",
    "        if not auditer:\n",
    "            raise ValueError(\"Cannot run PipelineBuiler without an instance of auditer\")\n",
    "\n",
    "        self.auditer = auditer\n",
    "    \n",
    "    def _inspect_pipelines(self):\n",
    "        \"\"\"\n",
    "        Display all pipelines stored in the _blueprints.\n",
    "        \"\"\"\n",
    "        if not self._blueprints:\n",
    "            print(\"No pipelines have been created yet.\")\n",
    "        else:\n",
    "            print(\"Current Pipelines:\")\n",
    "            for name, steps in self._blueprints.items():\n",
    "                print(f\"Pipeline '{name}':\")\n",
    "                for idx, step in enumerate(steps['sections'], 1):\n",
    "                    print(f\"  {idx}. {step}\")\n",
    "    \n",
    "        eligible_params = self._get_class_params(section_class)\n",
    "        return True, section_class, eligible_params\n",
    "\n",
    "    def _get_class_params(self, section_class):\n",
    "        \"\"\"\n",
    "        Retrieves eligible parameters for a class using inspection.\n",
    "        \"\"\"\n",
    "        params = inspect.signature(section_class).parameters\n",
    "        return list(params.keys())\n",
    "    \n",
    "    def _retrieve_class_from_scope(self, section_name):\n",
    "        \"\"\"\n",
    "        Retrieves the class object from global or local scope.\n",
    "        \"\"\"\n",
    "        section_class = globals().get(section_name) or locals().get(section_name)\n",
    "        if section_class is None:\n",
    "            print(f\"Error: '{section_name}' is not defined in the current scope. Try again.\")\n",
    "            return None\n",
    "        if not callable(section_class):\n",
    "            print(f\"Error: '{section_name}' is not callable. Try again.\")\n",
    "            return None\n",
    "        return section_class\n",
    "        \n",
    "    def _validate_section(self, section_name, is_predictor):\n",
    "        \"\"\"\n",
    "        Validates if a given section can be added to the pipeline.\n",
    "        \"\"\"\n",
    "        section_class = self._retrieve_class_from_scope(section_name)\n",
    "        if not section_class:\n",
    "            return False, None, []\n",
    "    \n",
    "        if is_predictor:\n",
    "            valid, section_class  = self._validate_predictor(section_class)\n",
    "        else:\n",
    "            valid, section_class = self._validate_transformer(section_class)\n",
    " \n",
    "        if not valid:\n",
    "            return False, None, []\n",
    "        else:\n",
    "            valid_params = self._get_class_params(section_class)\n",
    "            return valid, section_class, valid_params\n",
    "            \n",
    "    def _validate_predictor(self, section_class):\n",
    "        \"\"\"\n",
    "        Validates if the given class is a predictor.\n",
    "        \"\"\"\n",
    "        if not hasattr(section_class, 'predict') or \\\n",
    "           not any(base.__name__ in ['BaseEstimator', 'BaseEnsemble'] for base in inspect.getmro(section_class)):\n",
    "            print(f\"Error: '{section_class.__name__}' is not a valid predictor \"\n",
    "                  f\"(no 'predict' method or not derived from 'BaseEstimator' or 'BaseEnsemble'). Try again.\")\n",
    "            return False, None\n",
    "        return True, section_class\n",
    "\n",
    "    def _validate_transformer(self, section_class):\n",
    "        \"\"\"\n",
    "        Validates if the given class is a transformer.\n",
    "        \"\"\"\n",
    "        if not hasattr(section_class, 'transform') or \\\n",
    "           'TransformerMixin' not in [base.__name__ for base in inspect.getmro(section_class)]:\n",
    "            print(f\"Error: '{section_class.__name__}' is not a valid transformer \"\n",
    "                  f\"(no 'transform' method or not derived from 'TransformerMixin'). Try again.\")\n",
    "            return False, None\n",
    "        return True, section_class\n",
    "\n",
    "class IterPlumber(PipelineBuilder):\n",
    "    \"\"\"\n",
    "    Interactive pipeline builder allowing step-by-step creation and management of multiple pipelines.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_names=None, auditer=None):\n",
    "        if auditer is None:\n",
    "            raise ValueError(\"IterPlumber requires an instance of auditer to be provided.\")\n",
    "        super().__init__(column_names=column_names, auditer=auditer)\n",
    "\n",
    "        # Initialize available sections as an empty \n",
    "        self.available_sections = {\n",
    "            'transformers': {}, \n",
    "            'predictors' : {}\n",
    "        } # Tracks user-created components not yet in use\n",
    "        \n",
    "    def run_pipes(self):\n",
    "        \"\"\"\n",
    "        Main method to manage the creation of pipeline instructions for multiple pipelines iteratively.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            user_input = input(\n",
    "                \"\"\"\n",
    "    Welcome to IterPlumber! Please select an option:\n",
    "    1. Create a new pipeline\n",
    "    2. View current pipelines\n",
    "    3. Finalize blueprints\n",
    "    4. Cancel and exit\n",
    "    Your choice: \"\"\".strip()).strip()\n",
    "    \n",
    "            if user_input == '1':\n",
    "                # Initialize the current pipeline as an empty dictionary\n",
    "                print(\"\\nFollow the instructions to iteratively build a pipeline.\")\n",
    "                current_pipeline = self._build_pipeline()\n",
    "    \n",
    "                # If a pipeline was successfully built\n",
    "                if current_pipeline:\n",
    "                    pipeline_name = current_pipeline['name']\n",
    "                    self._blueprints[pipeline_name] = current_pipeline\n",
    "                    print(f\"Pipeline '{pipeline_name}' saved successfully.\")\n",
    "                else:\n",
    "                    print(\"Pipeline creation was cancelled or not completed.\")\n",
    "    \n",
    "            elif user_input == '2':\n",
    "                # View current pipelines\n",
    "                if self._blueprints:\n",
    "                    print(\"\\nCurrent pipelines:\")\n",
    "                    for name, pipeline in self._blueprints.items():\n",
    "                        print(f\"- {name}: {pipeline}\")\n",
    "                else:\n",
    "                    print(\"\\nNo pipelines have been created yet.\")\n",
    "    \n",
    "            elif user_input == '3':\n",
    "                # Finalize and save the pipelines\n",
    "                print(\"\\nFinalizing and saving pipelines...\")\n",
    "                self._finalize_blueprints()\n",
    "                print(\"Pipelines finalized successfully. Goodbye!\")\n",
    "                break\n",
    "    \n",
    "            elif user_input == '4':\n",
    "                # Cancel and exit\n",
    "                print(\"\\nCancelling all operations. Goodbye!\")\n",
    "                break\n",
    "    \n",
    "            else:\n",
    "                # Handle invalid input\n",
    "                print(\"\\nInvalid choice. Please select a valid option (1-4).\")\n",
    "\n",
    "\n",
    "    def _build_pipeline(self):\n",
    "        \"\"\"\n",
    "        Helper method to interactively build pipeline instructions for a single pipeline.\n",
    "        \"\"\"\n",
    "        self.current_column_names = {col: col for col in self.column_names}  # Keeps column mapping updated dynamically\n",
    "        \n",
    "        # Empty pipeline is instanced\n",
    "        section_id = 0\n",
    "        section_number = 0\n",
    "        \n",
    "        this_pipeline = {\n",
    "            'name': ''\n",
    "            , 'n_sections': 0\n",
    "            , 'sections': {}\n",
    "        }\n",
    "\n",
    "        # Flow controls for pipeline assembly\n",
    "        has_transformer = False\n",
    "        has_predictor = False\n",
    "    \n",
    "        while True:\n",
    "            print(\"\\n--- Pipeline Creation Menu ---\")\n",
    "            print(\"To build a pipeline, add at least one transformer and one predictor section.\"\n",
    "                  \"The last section must always be a predictor. Sections must be built before they can be added.\")\n",
    "            print(\"\\nOptions:\")\n",
    "            print(\"1. Build a transformer section\")\n",
    "            print(\"   - A transformer applies preprocessing steps to your data, such as scaling, encoding, or imputation.\")\n",
    "            print(\"2. Build a predictor\")\n",
    "            print(\"   - A predictor is the final step in the pipeline, such as a regression model or classifier.\")\n",
    "            print(\"3. Add a section to the pipeline\")\n",
    "            print(\"   - Use this to integrate a previously defined transformer or predictor into the pipeline.\")\n",
    "            print(\"4. View current pipeline\")\n",
    "            print(\"   - Displays the steps currently added to this pipeline, in the order they will be applied.\")\n",
    "            print(\"5. View current available sections\")\n",
    "            print(\"   - Displays the steps currently added to this pipeline, in the order they will be applied.\")\n",
    "            print(\"6. Finish and save this pipeline\")\n",
    "            print(\"   - Completes the pipeline creation process and saves the current pipeline.\")\n",
    "            print(\"7. Cancel this pipeline\")\n",
    "            print(\"   - Discards the current pipeline and returns to the main menu.\")\n",
    "            \n",
    "            choice = input(\"Enter your choice: \").strip()\n",
    "            \n",
    "            if choice == '1':\n",
    "                print(\"Building transformer section.\")\n",
    "                self._handle_section()\n",
    "        \n",
    "            elif choice == '2':\n",
    "                print(\"Building predictor.\")\n",
    "                self._handle_section(is_predictor=True)\n",
    "                \n",
    "            elif choice == '3':\n",
    "                if has_predictor:\n",
    "                    print('Unable to add more sections, pipeline already has a predictor')\n",
    "                    break\n",
    "                \n",
    "                if self.available_sections['transformers'] or self.available_sections['predictors']:\n",
    "                    while True:\n",
    "                        print(\"Adding section to the pipeline.\")\n",
    "                        print(\"Select which type of section you wish to add to the pipeline\")\n",
    "                        print(\"1. Add a transformer\")\n",
    "                        print(\"2. Add a predictor\")\n",
    "                        print(\"3. Add a column_transformer (requires a transformer)\")\n",
    "                        print(\"4. Go back to Pipeline creation menu.\")\n",
    "    \n",
    "                        self._view_sections()\n",
    "                        \n",
    "                        choice = input(\"Enter your choice: \").strip()\n",
    "                        if choice == '1':\n",
    "                            if self.available_sections['transformers']:\n",
    "                                has_transformer = self._add_section(this_pipeline, is_predictor=False)\n",
    "                            else:\n",
    "                                print('No transfomers available to add.')\n",
    "                            \n",
    "                        elif choice == '2':\n",
    "                            if self.available_sections['predictors']:\n",
    "                                has_predictor = self._add_section(this_pipeline, is_predictor=True)\n",
    "                            else:\n",
    "                                print('No predictors available to add.')\n",
    "                        elif choice == '3':\n",
    "                            if self.available_sections['transformers']:\n",
    "                                has_transformer = self._add_section(this_pipeline, is_column_transformer=True)\n",
    "\n",
    "                        elif choice == '4':\n",
    "                            print('Returning to previous menu.')\n",
    "                            break\n",
    "                        else:\n",
    "                            print('No transfomers available to add.')\n",
    "                            \n",
    "                else:\n",
    "                    print(\"No pipes available to add.\")\n",
    "                \n",
    "            elif choice == '4':\n",
    "                self._view_pipeline(this_pipeline)\n",
    "                \n",
    "            elif choice == '5':\n",
    "                self._view_sections()\n",
    "                \n",
    "            elif choice == '6':\n",
    "                if has_predictor:\n",
    "                    predictor_key = max(this_pipeline['sections'].keys())\n",
    "                    this_pipeline['name'] = this_pipeline['sections'][predictor_key]['name']\n",
    "                \n",
    "                    print(\"Saving the current pipeline with predictor:\", this_pipeline['name'] )\n",
    "                    return this_pipeline\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Cannot store empty or incomplete pipeline blueprints. Please try again.\")\n",
    "                    \n",
    "            elif choice == '7': \n",
    "                print(\"Cancelling the current pipeline and returning to the main menu.\")\n",
    "                input(\"Press any key to continue\")\n",
    "                return None\n",
    "                \n",
    "            else:\n",
    "                print(\"Invalid choice. Please try again.\")\n",
    "            \n",
    "            input(\"Press any key to continue\")\n",
    "        \n",
    "                \n",
    "    def _handle_section(self, is_predictor=False):\n",
    "        \"\"\"Handles flow into _build_section which itself provides an iterative procedure to generate pipelines, receives a section in return and parses it into the available_sections\"\"\"\n",
    "        if is_predictor:\n",
    "            string = \"predictors\"\n",
    "        else:\n",
    "            string = \"transformers\"\n",
    "        \n",
    "        section_name = input(f\"Enter the {string} class name: \").strip()   \n",
    "\n",
    "        if section_name.lower() in self.available_sections[string]:\n",
    "            print(f'Failed to create {section_name}. Another section with that name already exists')\n",
    "            valid = False\n",
    "        else:\n",
    "            valid, section_class, eligible_params = self._validate_section(section_name, is_predictor)\n",
    "        \n",
    "        if valid:\n",
    "            section = self._build_section(section_name, section_class, eligible_params)\n",
    "\n",
    "            self.available_sections[string][section['_name']] = {\n",
    "                'name': section['_name'],\n",
    "                'class': section['_class'],\n",
    "                'args': section['_args'],\n",
    "                'grid': section['_grid'],\n",
    "                'columns' : section['_columns']\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            input(f\"Failed to build {string} section. Press any key to return to the previous menu\")\n",
    "                \n",
    "    def _build_section(self, section_name, section_class, eligible_params):\n",
    "        \"\"\"\n",
    "        Prompts the user to provide values for eligible parameters of a section\n",
    "        and validates them by instantiating the class and calling its `fit` method.\n",
    "        \"\"\"\n",
    "        # Try to print documentation to assist the user\n",
    "        try:\n",
    "            print(section_class.__doc__)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nUnexpected issue with docstring: {e}\")\n",
    "    \n",
    "        print(f\"Now building '{section_name.lower()}'. Ensure that parameter values are correct (Docstring above).\")\n",
    "    \n",
    "        raw_params, params = {}, {}\n",
    "        raw_grid_params, grid_params = {}, {}\n",
    "         \n",
    "\n",
    "        for param in eligible_params:\n",
    "            user_input = input(f\"Enter a value for '{param}' (or an iterable like a list or range for grid search if applicable): \")\n",
    "            try:\n",
    "                if user_input == '':\n",
    "                    user_input = None\n",
    "                else:\n",
    "                    parsed_input = eval(user_input)\n",
    "            except (ValueError, SyntaxError, NameError):\n",
    "                parsed_input = user_input\n",
    "        \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if isinstance(parsed_input, (dict, list, Iterable)) and not isinstance(parsed_input, str):\n",
    "                raw_grid_params[param], grid_params[param] = user_input, parsed_input\n",
    "            else:\n",
    "                raw_params[param], params[param] = user_input, parsed_input\n",
    "                \n",
    "        if not grid_params:\n",
    "                max_iterations = 1\n",
    "                grid_iterators = {}\n",
    "        else:\n",
    "            grid_iterators = {param: iter(values) for param, values in grid_params.items()}\n",
    "                 \n",
    "        # Testing loop\n",
    "        number_iterators = len(grid_params.keys())\n",
    "        number_stop_iterators = 0\n",
    "        current_args = params.copy()\n",
    "        \n",
    "        try:\n",
    "            while number_stop_iterators != number_iterators:\n",
    "                number_stop_iterators = 0\n",
    "                for param, iterator in grid_iterators.items():\n",
    "                    try:\n",
    "                        current_args[param] = next(iterator)\n",
    "                    except StopIteration:\n",
    "                        number_stop_iterators += 1\n",
    "                    \n",
    "                print(f\"Testing with arguments: {current_args}\")\n",
    "                dummy_data_x, dummy_data_y = [[0]], [[0]]  # Placeholder data\n",
    "                section_instance = section_class(**current_args)\n",
    "                try:\n",
    "                    section_instance.fit(dummy_data_x)  # Validate the instance\n",
    "                except Exception as e:\n",
    "                    section_instance.fit(dummy_data_x, dummy_data_y)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed with arguments {current_args}: {e}\")\n",
    "            retry = input(\"Invalid arguments detected, try again? (y/n): \").strip().lower()\n",
    "            if retry != 'y':\n",
    "                print(\"Exiting pipeline creation.\")\n",
    "                return None\n",
    "            else:\n",
    "                # Retry the entire section\n",
    "                return self._build_section(section_name, section_class, eligible_params)\n",
    "    \n",
    "        print(f\"Successfully tested all arguments,\")\n",
    "        section = {\n",
    "            '_name': section_class.__name__.lower(),\n",
    "            '_class': section_class.__name__,\n",
    "            '_args': raw_params,\n",
    "            '_grid': raw_grid_params,\n",
    "            '_columns': []\n",
    "        }\n",
    "    \n",
    "        return section\n",
    "\n",
    "    \n",
    "    def _add_section(self, pipeline, is_predictor=False, is_column_transformer=False):\n",
    "        \"\"\"\n",
    "        Add a section to the pipeline, either as a predictor or a transformer.\n",
    "        Handles column selection for column transformers iteratively.\n",
    "    \n",
    "        Args:\n",
    "            pipeline (dict): The current pipeline being built.\n",
    "            is_predictor (bool): Whether the section is a predictor. Defaults to False.\n",
    "            is_column_transformer (bool): Whether the section is a column transformer. Defaults to False.\n",
    "    \n",
    "        Returns:\n",
    "            bool: True if at least one section was added, False otherwise.\n",
    "        \"\"\"\n",
    "        section_type = 'predictors' if is_predictor else 'transformers'\n",
    "        selectable_sections = self.available_sections[section_type].copy()\n",
    "        selected_sections = {}\n",
    "        \n",
    "        # For column transformers, maintain a list of selectable columns\n",
    "        selectable_columns = self.column_names.copy() if is_column_transformer else None\n",
    "        has_more_to_add = True\n",
    "        at_least_one_selected = False\n",
    "        section_number = 0  # Internal section number (increments for column transformers)\n",
    "    \n",
    "        while has_more_to_add:\n",
    "            # Step 1: Handle column selection if necessary\n",
    "            selected_columns = None\n",
    "            if is_column_transformer:\n",
    "                selected_columns = self._select_columns(selectable_columns)\n",
    "                if not selected_columns:\n",
    "                    print(\"No columns selected. Returning to the previous menu.\")\n",
    "                    return False\n",
    "                # Update the remaining selectable columns\n",
    "                selectable_columns = [col for col in selectable_columns if col not in selected_columns]\n",
    "    \n",
    "            # Step 2: Handle section selection\n",
    "            section_name, section_details = self._select_section(selectable_sections)\n",
    "            if not section_name:\n",
    "                print(\"No section selected. Returning to the previous menu.\")\n",
    "                return False\n",
    "                        \n",
    "            # Add columns if this is a column transformer\n",
    "            if is_column_transformer:\n",
    "                section_details['name'] = section_name\n",
    "                section_details['columns'] = selected_columns\n",
    "                print(f\"Assigned columns {selected_columns} to {section_name}.\")\n",
    "                \n",
    "            \n",
    "            # Increment for each section selected\n",
    "            section_number += 1  \n",
    "            \n",
    "            # Store the selected section\n",
    "            selected_sections[section_number] = section_details\n",
    "            selectable_sections.pop(section_name, None)  # Remove from available sections\n",
    "            \n",
    "            at_least_one_selected = True\n",
    "    \n",
    "            # Step 3: Decide whether to add more transformers to this column transformer\n",
    "            if is_column_transformer and selectable_columns and selectable_sections:\n",
    "                add_more = input(\"Add another transformer to the column transformer? (y/n): \").strip().lower()\n",
    "                if add_more != 'y':\n",
    "                    print(\"Finalizing current column transformer.\")\n",
    "                    has_more_to_add = False\n",
    "            else:\n",
    "                has_more_to_add = False  # No more sections or columns to add\n",
    "    \n",
    "        if at_least_one_selected:\n",
    "            # Update the id before incrementing\n",
    "            section_id = pipeline['n_sections'] + 1\n",
    "                \n",
    "            # Step 4: Add column transformer or section to the pipeline\n",
    "            if is_column_transformer:\n",
    "                # Create a composite name for the column transformer\n",
    "                transformer_name = '_'.join(\n",
    "                    ['column_transformer'] + [\n",
    "                        selected_sections[section_number]['name'] for section_number in selected_sections\n",
    "                    ]\n",
    "                )\n",
    "                column_transformer = {\n",
    "                    'name': transformer_name,\n",
    "                    'transformers': selected_sections\n",
    "                }\n",
    "                # Add the column transformer to the pipeline\n",
    "                pipeline['sections'][section_id] = column_transformer\n",
    "\n",
    "            else:\n",
    "                # Add single predictor/transformer to the pipeline\n",
    "                section_number = next(iter(selected_sections))  # Extract the single selected section\n",
    "                pipeline['sections'][section_id] = selected_sections[section_number]\n",
    "            \n",
    "            # Remove selected sections from available_sections\n",
    "            for section_number in selected_sections:\n",
    "                del self.available_sections[section_type][selected_sections[section_number]['name']]\n",
    "\n",
    "            # Increment the number of sections\n",
    "            pipeline['n_sections'] += 1\n",
    "        \n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(\"No sections were successfully added to the pipeline.\")\n",
    "            return False\n",
    "\n",
    "    def _select_section(self, available_sections):\n",
    "        while True:\n",
    "            print(f\"Available sections: \\n{available_sections.keys()}\")\n",
    "            chosen_section = input(\"Select a transformer by entering its name (see above)\")\n",
    "            \n",
    "            if chosen_section not in available_sections.keys():\n",
    "                print(\"Failed to match your input with an available section\")\n",
    "                retry = input(\"Try again y/n?\")\n",
    "                if retry == 'y':\n",
    "                    continue\n",
    "                else:\n",
    "                    print('Returning to the previous menu')\n",
    "                    return None, None\n",
    "            else:\n",
    "                return chosen_section, available_sections[chosen_section]\n",
    "                    \n",
    "    def _select_columns(self, remaining_columns):\n",
    "        \"\"\"\n",
    "        Prompt the user to select specific columns for the given transformer section using indices,\n",
    "        ranges, or slices. Allows the user to cancel and return to the previous menu.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Display available columns with indices\n",
    "            print(\"\\nAvailable columns:\")\n",
    "            for idx, col in enumerate(remaining_columns, start=1):\n",
    "                print(f\"{idx}. {col}\")\n",
    "            \n",
    "            print(\"\\nEnter column indices, ranges (e.g., range(1, 5)), or slices (e.g., 1:5).\")\n",
    "            print(\"Type 'cancel' to return to the previous menu.\")\n",
    "            \n",
    "            # Get user input\n",
    "            user_input = input(\"Enter indices, ranges, or 'cancel': \").strip()\n",
    "            \n",
    "            # Handle cancel option\n",
    "            if user_input.lower() == 'cancel':\n",
    "                print(\"Returning to the previous menu.\")\n",
    "                return None\n",
    "            \n",
    "            try:\n",
    "                # Try to evaluate the entire input as an iterable\n",
    "                selected_indices = []\n",
    "                try:\n",
    "                    evaluated = eval(user_input)\n",
    "                    if isinstance(evaluated, Iterable):\n",
    "                        selected_indices.extend(list(evaluated))\n",
    "                    else:\n",
    "                        raise ValueError(\"Input is not an iterable.\")\n",
    "                except Exception:\n",
    "                    # If not an iterable, split and process parts individually\n",
    "                    inputs = [item.strip() for item in user_input.split(',')]\n",
    "                    for item in inputs:\n",
    "                        selected_indices.extend(list(eval(item)))\n",
    "                \n",
    "                # Deduplicate and validate indices\n",
    "                selected_indices = sorted(set(selected_indices))  # Remove duplicates\n",
    "                if not all(1 <= idx <= len(remaining_columns) for idx in selected_indices):\n",
    "                    print(f\"Invalid indices. Please ensure values are between 1 and {len(remaining_columns)}.\")\n",
    "                    continue\n",
    "                \n",
    "                # Translate indices to column names\n",
    "                selected_columns = [remaining_columns[idx - 1] for idx in selected_indices]\n",
    "                \n",
    "                # Return the selected columns\n",
    "                return selected_columns\n",
    "            except Exception as e:\n",
    "                print(f\"Invalid input: {e}. Please enter valid indices, ranges, or slices.\")\n",
    "    \n",
    "    def _view_sections(self):\n",
    "        for section_type in self.available_sections.items():\n",
    "            if not section_type:\n",
    "                print(f\"No sections available for type {section_type}\")\n",
    "            else:\n",
    "                for section in section_type:\n",
    "                    print(section),\n",
    "                print()  \n",
    "                \n",
    "    def _view_pipeline(self, this_pipeline):\n",
    "        if not this_pipeline:\n",
    "            print(\" No sections in use/available.\")\n",
    "        else:\n",
    "            print(\"Current pipeline:\")\n",
    "            for step_name, step_details in this_pipeline['sections'].items():\n",
    "                print(f\"- {step_name}: {step_details}\")\n",
    "\n",
    "    def _finalize_blueprints(self):\n",
    "        \"\"\"\n",
    "        Finalize all blueprints by saving them using an external auditor.\n",
    "        \"\"\"\n",
    "        if not self._blueprints:\n",
    "            print(\"No pipelines were created. Nothing to save.\")\n",
    "            return\n",
    "    \n",
    "        print(\"\\nFinalizing and saving all created pipelines...\")\n",
    "        # Call auditer to sign the blueprint to the working directory\n",
    "        self.auditer.sign_and_save_config(self._blueprints)\n",
    "        print(\"Pipelines saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d384007-2b02-44c8-bd87-cc00c212b0fa",
   "metadata": {},
   "source": [
    "# Fitter\n",
    "\n",
    "Trains models using instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9e579b9d-65e4-428e-bed7-0e220c4192bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter:\n",
    "    \n",
    "    def __init__(self, X, y, instructions_json, logger):\n",
    "        \n",
    "        if not instructions_json or not logger:\n",
    "            raise ValueError(\"Both 'instructions_json' and 'logger' must be provided.\")\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.ml_logger = logger\n",
    "        \n",
    "        # Parse blueprints into pipelines and grids\n",
    "        self.instructions = self.lay_pipeline(instructions_json.copy())\n",
    "\n",
    "        print(self.instructions)\n",
    "        \n",
    "        self.outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        self.inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        self.results_buffer = []\n",
    "\n",
    "    def lay_pipeline(self, json_blueprints):\n",
    "        \n",
    "        pipelines_and_grids = []\n",
    "    \n",
    "        for blueprint_name, blueprint_data in json_blueprints.items():\n",
    "            if blueprint_name == 'config_id':\n",
    "                continue\n",
    "    \n",
    "            sections = blueprint_data['sections']\n",
    "            steps = []\n",
    "            grid_params = {}\n",
    "    \n",
    "            for section_id, section_info in sections.items():\n",
    "                if 'transformers' in section_info:  # Column Transformer\n",
    "                    column_transformer_steps = []\n",
    "                    for transformer in section_info['transformers'].values():\n",
    "                        # Transformer step details\n",
    "                        transformer_name = transformer['name']\n",
    "                        transformer_class = eval(transformer['class'])(**transformer['args'])\n",
    "                        transformer_columns = transformer['columns']\n",
    "    \n",
    "                        # Add transformer to ColumnTransformer steps\n",
    "                        column_transformer_steps.append((transformer_name, transformer_class, transformer_columns))\n",
    "    \n",
    "                        # Extract grid parameters for the transformer\n",
    "                        for param_key, param_values in transformer['grid'].items():\n",
    "                            grid_key = f\"{section_info['name']}__{transformer_name}__{param_key}\"\n",
    "                            grid_params[grid_key] = eval(param_values)\n",
    "    \n",
    "                    # Add the ColumnTransformer to the pipeline\n",
    "                    steps.append(\n",
    "                        (section_info['name'], ColumnTransformer(column_transformer_steps))\n",
    "                    )\n",
    "                else:  # Model/Predictor\n",
    "                    step_name = section_info['name']\n",
    "                    model_instance = eval(section_info['class'])(**section_info['args'])\n",
    "                    steps.append((step_name, model_instance))\n",
    "    \n",
    "                    # Extract grid parameters for the model\n",
    "                    for param_key, param_values in section_info['grid'].items():\n",
    "                        grid_key = f\"{step_name}__{param_key}\"\n",
    "                        grid_params[grid_key] = eval(param_values)\n",
    "    \n",
    "            # Create the pipeline\n",
    "            pipeline = Pipeline(steps)\n",
    "            pipelines_and_grids.append((blueprint_name, pipeline, grid_params))\n",
    "    \n",
    "        return pipelines_and_grids\n",
    "\n",
    "\n",
    "    def process_and_store_results(self, model_name, df_results, best_fold_index):\n",
    "        \"\"\"\n",
    "        Processes cross-validation results and stores them in a structured format.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the model.\n",
    "            df_results (dict): Cross-validation results.\n",
    "            best_fold_index (int): Index of the best scoring fold.\n",
    "\n",
    "        Returns:\n",
    "            dict: Processed results for the model.\n",
    "        \"\"\"\n",
    "        best_fold_estimator = df_results['estimator'][best_fold_index]\n",
    "        best_score = df_results['test_score'][best_fold_index]\n",
    "        best_params = best_fold_estimator.best_params_\n",
    "\n",
    "        mean_train_score = df_results['train_score'].mean()\n",
    "        mean_test_score = df_results['test_score'].mean()\n",
    "        fit_time = df_results['fit_time'][best_fold_index]\n",
    "        score_time = df_results['score_time'][best_fold_index]\n",
    "\n",
    "        best_fold_estimator.fit(self.X, self.y)\n",
    "        refit_train_score = best_fold_estimator.score(self.X, self.y)\n",
    "\n",
    "        model_result = {\n",
    "            \"model\": model_name,\n",
    "            \"best_hyperparameters\": best_params,\n",
    "            \"best_test_score\": best_score,\n",
    "            \"mean_train_score\": mean_train_score,\n",
    "            \"mean_test_score\": mean_test_score,\n",
    "            \"fit_time\": fit_time,\n",
    "            \"score_time\": score_time,\n",
    "            \"refit_train_score\": refit_train_score,\n",
    "        }\n",
    "\n",
    "        self.results_buffer.append(model_result)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executes the fitting process for all instructions and handles results storage.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for model_name, pipeline, params in self.instructions:     \n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=pipeline,\n",
    "                    param_grid=params,  # Param grid from last step\n",
    "                    scoring='f1_macro',\n",
    "                    cv=self.inner_cv,\n",
    "                    n_jobs=1,\n",
    "                    refit=True,\n",
    "                    verbose=2         \n",
    "                )\n",
    "    \n",
    "                cv_results = cross_validate(\n",
    "                    estimator=grid_search,\n",
    "                    X=self.X,\n",
    "                    y=self.y,\n",
    "                    cv=self.outer_cv,\n",
    "                    return_train_score=True,\n",
    "                    return_estimator=True,\n",
    "                    scoring=\"f1_macro\",\n",
    "                    n_jobs=1,\n",
    "                    verbose=2\n",
    "                )\n",
    "    \n",
    "                best_fold_index = cv_results['test_score'].argmax()\n",
    "                self.process_and_store_results(model_name, cv_results, best_fold_index)\n",
    "        \n",
    "        finally:\n",
    "            self.ml_logger.results_to_csv(self.results_buffer)\n",
    "            print(\"All results saved by the logger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea79d6-b9b2-4d0a-969f-9ad9524754c6",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b6034e5-2146-4ed7-9e0d-a1dd3aa0bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the base directory for the process\n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9dc7f1-29e9-4df8-971c-52fa76114df4",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "Use the cell below to load a data file."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7198b840-995d-4046-887e-67c7493e8ecb",
   "metadata": {},
   "source": [
    "# Load data here\n",
    "# Example with Iris\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target)\n",
    "\n",
    "columns = iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "59d9469e-7556-4f39-9f0e-976ed5226828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is ingested from the working directory, with the index set to col_0 and dtypes applied directly\n",
    "with open('../data/dtypes.json', 'r') as file:\n",
    "    train = pd.read_csv('../data/preproc_train.csv', index_col=0, dtype=json.load(file), low_memory=True)\n",
    "\n",
    "# Data is ingested from the working directory, with the index set to col_0 and dtypes applied directly\n",
    "with open('../data/dtypes.json', 'r') as file:\n",
    "    test = pd.read_csv('../data/preproc_test.csv', index_col=0, dtype=json.load(file), low_memory=True)\n",
    "\n",
    "# Convert all NaN values to np.nan\n",
    "train = train.fillna(np.nan)\n",
    "\n",
    "# Some shortcuts for now,\n",
    "X = train.iloc[:, :-1].copy()\n",
    "y = train.iloc[:, -1]\n",
    "\n",
    "# Drop missing from y\n",
    "y = y.dropna()\n",
    "\n",
    "# Align X and y based on y's indices\n",
    "X = X.loc[y.index]\n",
    "\n",
    "# Now select numeric columns for X, and sample 20k for ease of testing\n",
    "X = X.select_dtypes(include='number').sample(n=20000)\n",
    "\n",
    "# Slice y on the selected X indices\n",
    "y = y[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c6ad4fa-09d1-444d-80ce-d6705f09637f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_at_injury</th>\n",
       "      <th>alternative_dispute_resolution</th>\n",
       "      <th>attorney_representative</th>\n",
       "      <th>average_weekly_wage</th>\n",
       "      <th>covid_19_indicator</th>\n",
       "      <th>gender</th>\n",
       "      <th>ime_4_count</th>\n",
       "      <th>missing_accident_date</th>\n",
       "      <th>missing_age_at_injury</th>\n",
       "      <th>missing_average_weekly_wage</th>\n",
       "      <th>...</th>\n",
       "      <th>assembly_date_month</th>\n",
       "      <th>assembly_date_day</th>\n",
       "      <th>c_2_date_year</th>\n",
       "      <th>c_2_date_month</th>\n",
       "      <th>c_2_date_day</th>\n",
       "      <th>c_3_date_year</th>\n",
       "      <th>c_3_date_month</th>\n",
       "      <th>c_3_date_day</th>\n",
       "      <th>age_at_injury_zero</th>\n",
       "      <th>is_unionized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim_identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5460667</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5845717</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>713.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620478</th>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161798</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6136675</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  age_at_injury  alternative_dispute_resolution  \\\n",
       "claim_identifier                                                  \n",
       "5460667                    54.0                             0.0   \n",
       "5845717                    48.0                             0.0   \n",
       "5620478                    53.0                             0.0   \n",
       "6161798                    54.0                             0.0   \n",
       "6136675                    26.0                             0.0   \n",
       "\n",
       "                  attorney_representative  average_weekly_wage  \\\n",
       "claim_identifier                                                 \n",
       "5460667                               0.0                 0.00   \n",
       "5845717                               0.0               713.86   \n",
       "5620478                               0.0                 0.00   \n",
       "6161798                               0.0                 0.00   \n",
       "6136675                               0.0                 0.00   \n",
       "\n",
       "                  covid_19_indicator  gender  ime_4_count  \\\n",
       "claim_identifier                                            \n",
       "5460667                          1.0     1.0          0.0   \n",
       "5845717                          0.0     1.0          0.0   \n",
       "5620478                          0.0     1.0          0.0   \n",
       "6161798                          0.0     0.0          0.0   \n",
       "6136675                          0.0     NaN          0.0   \n",
       "\n",
       "                  missing_accident_date  missing_age_at_injury  \\\n",
       "claim_identifier                                                 \n",
       "5460667                             0.0                    0.0   \n",
       "5845717                             0.0                    0.0   \n",
       "5620478                             0.0                    0.0   \n",
       "6161798                             0.0                    0.0   \n",
       "6136675                             0.0                    0.0   \n",
       "\n",
       "                  missing_average_weekly_wage  ...  assembly_date_month  \\\n",
       "claim_identifier                               ...                        \n",
       "5460667                                   0.0  ...                  3.0   \n",
       "5845717                                   0.0  ...                 10.0   \n",
       "5620478                                   0.0  ...                 12.0   \n",
       "6161798                                   0.0  ...                 12.0   \n",
       "6136675                                   0.0  ...                 11.0   \n",
       "\n",
       "                  assembly_date_day  c_2_date_year  c_2_date_month  \\\n",
       "claim_identifier                                                     \n",
       "5460667                        30.0         2020.0             3.0   \n",
       "5845717                        28.0         2021.0            10.0   \n",
       "5620478                        17.0         2020.0            12.0   \n",
       "6161798                        23.0         2022.0            12.0   \n",
       "6136675                        18.0         2022.0            11.0   \n",
       "\n",
       "                  c_2_date_day  c_3_date_year  c_3_date_month  c_3_date_day  \\\n",
       "claim_identifier                                                              \n",
       "5460667                   30.0         2020.0             4.0           2.0   \n",
       "5845717                   28.0         2021.0            10.0          31.0   \n",
       "5620478                   17.0         2020.0            12.0          20.0   \n",
       "6161798                   23.0         2022.0            12.0          26.0   \n",
       "6136675                   18.0         2022.0            11.0          21.0   \n",
       "\n",
       "                  age_at_injury_zero  is_unionized  \n",
       "claim_identifier                                    \n",
       "5460667                          0.0           0.0  \n",
       "5845717                          0.0           0.0  \n",
       "5620478                          0.0           0.0  \n",
       "6161798                          0.0           0.0  \n",
       "6136675                          0.0           0.0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eea55b9f-38d9-4ad7-9439-c4e89176763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1414\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X.isna().sum().sum())  # Total count of NaNs in X\n",
    "print(y.isna().sum())  # Total count of NaNs in y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb707a-e112-446b-a091-7796b9357243",
   "metadata": {},
   "source": [
    "## Generate instructions\n",
    "\n",
    "Import all necessary libraries here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "573a5a99-b9a6-4480-beca-13214711a06f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transformer imports\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Predictor imports\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7da330c-8012-46a6-9521-5e75a8ccd3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an instance of the Auditer\n",
    "auditer = MLAuditer(base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d988ec66-9f7c-4e91-ac8c-98c1d363d222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Welcome to IterPlumber! Please select an option:\n",
      "    1. Create a new pipeline\n",
      "    2. View current pipelines\n",
      "    3. Finalize blueprints\n",
      "    4. Cancel and exit\n",
      "    Your choice: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Follow the instructions to iteratively build a pipeline.\n",
      "\n",
      "--- Pipeline Creation Menu ---\n",
      "To build a pipeline, add at least one transformer and one predictor section.The last section must always be a predictor. Sections must be built before they can be added.\n",
      "\n",
      "Options:\n",
      "1. Build a transformer section\n",
      "   - A transformer applies preprocessing steps to your data, such as scaling, encoding, or imputation.\n",
      "2. Build a predictor\n",
      "   - A predictor is the final step in the pipeline, such as a regression model or classifier.\n",
      "3. Add a section to the pipeline\n",
      "   - Use this to integrate a previously defined transformer or predictor into the pipeline.\n",
      "4. View current pipeline\n",
      "   - Displays the steps currently added to this pipeline, in the order they will be applied.\n",
      "5. View current available sections\n",
      "   - Displays the steps currently added to this pipeline, in the order they will be applied.\n",
      "6. Finish and save this pipeline\n",
      "   - Completes the pipeline creation process and saves the current pipeline.\n",
      "7. Cancel this pipeline\n",
      "   - Discards the current pipeline and returns to the main menu.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building transformer section.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the transformers class name:  SimpleImputer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univariate imputer for completing missing values with simple strategies.\n",
      "\n",
      "    Replace missing values using a descriptive statistic (e.g. mean, median, or\n",
      "    most frequent) along each column, or using a constant value.\n",
      "\n",
      "    Read more in the :ref:`User Guide <impute>`.\n",
      "\n",
      "    .. versionadded:: 0.20\n",
      "       `SimpleImputer` replaces the previous `sklearn.preprocessing.Imputer`\n",
      "       estimator which is now removed.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    missing_values : int, float, str, np.nan, None or pandas.NA, default=np.nan\n",
      "        The placeholder for the missing values. All occurrences of\n",
      "        `missing_values` will be imputed. For pandas' dataframes with\n",
      "        nullable integer dtypes with missing values, `missing_values`\n",
      "        can be set to either `np.nan` or `pd.NA`.\n",
      "\n",
      "    strategy : str or Callable, default='mean'\n",
      "        The imputation strategy.\n",
      "\n",
      "        - If \"mean\", then replace missing values using the mean along\n",
      "          each column. Can only be used with numeric data.\n",
      "        - If \"median\", then replace missing values using the median along\n",
      "          each column. Can only be used with numeric data.\n",
      "        - If \"most_frequent\", then replace missing using the most frequent\n",
      "          value along each column. Can be used with strings or numeric data.\n",
      "          If there is more than one such value, only the smallest is returned.\n",
      "        - If \"constant\", then replace missing values with fill_value. Can be\n",
      "          used with strings or numeric data.\n",
      "        - If an instance of Callable, then replace missing values using the\n",
      "          scalar statistic returned by running the callable over a dense 1d\n",
      "          array containing non-missing values of each column.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "           strategy=\"constant\" for fixed value imputation.\n",
      "\n",
      "        .. versionadded:: 1.5\n",
      "           strategy=callable for custom value imputation.\n",
      "\n",
      "    fill_value : str or numerical value, default=None\n",
      "        When strategy == \"constant\", `fill_value` is used to replace all\n",
      "        occurrences of missing_values. For string or object data types,\n",
      "        `fill_value` must be a string.\n",
      "        If `None`, `fill_value` will be 0 when imputing numerical\n",
      "        data and \"missing_value\" for strings or object data types.\n",
      "\n",
      "    copy : bool, default=True\n",
      "        If True, a copy of X will be created. If False, imputation will\n",
      "        be done in-place whenever possible. Note that, in the following cases,\n",
      "        a new copy will always be made, even if `copy=False`:\n",
      "\n",
      "        - If `X` is not an array of floating values;\n",
      "        - If `X` is encoded as a CSR matrix;\n",
      "        - If `add_indicator=True`.\n",
      "\n",
      "    add_indicator : bool, default=False\n",
      "        If True, a :class:`MissingIndicator` transform will stack onto output\n",
      "        of the imputer's transform. This allows a predictive estimator\n",
      "        to account for missingness despite imputation. If a feature has no\n",
      "        missing values at fit/train time, the feature won't appear on\n",
      "        the missing indicator even if there are missing values at\n",
      "        transform/test time.\n",
      "\n",
      "    keep_empty_features : bool, default=False\n",
      "        If True, features that consist exclusively of missing values when\n",
      "        `fit` is called are returned in results when `transform` is called.\n",
      "        The imputed value is always `0` except when `strategy=\"constant\"`\n",
      "        in which case `fill_value` will be used instead.\n",
      "\n",
      "        .. versionadded:: 1.2\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    statistics_ : array of shape (n_features,)\n",
      "        The imputation fill value for each feature.\n",
      "        Computing statistics can result in `np.nan` values.\n",
      "        During :meth:`transform`, features corresponding to `np.nan`\n",
      "        statistics will be discarded.\n",
      "\n",
      "    indicator_ : :class:`~sklearn.impute.MissingIndicator`\n",
      "        Indicator used to add binary indicators for missing values.\n",
      "        `None` if `add_indicator=False`.\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    IterativeImputer : Multivariate imputer that estimates values to impute for\n",
      "        each feature with missing values from all the others.\n",
      "    KNNImputer : Multivariate imputer that estimates missing features using\n",
      "        nearest samples.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    Columns which only contained missing values at :meth:`fit` are discarded\n",
      "    upon :meth:`transform` if strategy is not `\"constant\"`.\n",
      "\n",
      "    In a prediction context, simple imputation usually performs poorly when\n",
      "    associated with a weak learner. However, with a powerful learner, it can\n",
      "    lead to as good or better performance than complex imputation such as\n",
      "    :class:`~sklearn.impute.IterativeImputer` or :class:`~sklearn.impute.KNNImputer`.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.impute import SimpleImputer\n",
      "    >>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "    >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
      "    SimpleImputer()\n",
      "    >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n",
      "    >>> print(imp_mean.transform(X))\n",
      "    [[ 7.   2.   3. ]\n",
      "     [ 4.   3.5  6. ]\n",
      "     [10.   3.5  9. ]]\n",
      "\n",
      "    For a more detailed example see\n",
      "    :ref:`sphx_glr_auto_examples_impute_plot_missing_values.py`.\n",
      "    \n",
      "Now building 'simpleimputer'. Ensure that parameter values are correct (Docstring above).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a value for 'missing_values' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'strategy' (or an iterable like a list or range for grid search if applicable):  median\n",
      "Enter a value for 'fill_value' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'copy' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'add_indicator' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'keep_empty_features' (or an iterable like a list or range for grid search if applicable):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully tested all arguments,\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press any key to continue \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pipeline Creation Menu ---\n",
      "To build a pipeline, add at least one transformer and one predictor section.The last section must always be a predictor. Sections must be built before they can be added.\n",
      "\n",
      "Options:\n",
      "1. Build a transformer section\n",
      "   - A transformer applies preprocessing steps to your data, such as scaling, encoding, or imputation.\n",
      "2. Build a predictor\n",
      "   - A predictor is the final step in the pipeline, such as a regression model or classifier.\n",
      "3. Add a section to the pipeline\n",
      "   - Use this to integrate a previously defined transformer or predictor into the pipeline.\n",
      "4. View current pipeline\n",
      "   - Displays the steps currently added to this pipeline, in the order they will be applied.\n",
      "5. View current available sections\n",
      "   - Displays the steps currently added to this pipeline, in the order they will be applied.\n",
      "6. Finish and save this pipeline\n",
      "   - Completes the pipeline creation process and saves the current pipeline.\n",
      "7. Cancel this pipeline\n",
      "   - Discards the current pipeline and returns to the main menu.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building transformer section.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the transformers class name:  MinMaxScaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform features by scaling each feature to a given range.\n",
      "\n",
      "    This estimator scales and translates each feature individually such\n",
      "    that it is in the given range on the training set, e.g. between\n",
      "    zero and one.\n",
      "\n",
      "    The transformation is given by::\n",
      "\n",
      "        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      "        X_scaled = X_std * (max - min) + min\n",
      "\n",
      "    where min, max = feature_range.\n",
      "\n",
      "    This transformation is often used as an alternative to zero mean,\n",
      "    unit variance scaling.\n",
      "\n",
      "    `MinMaxScaler` doesn't reduce the effect of outliers, but it linearly\n",
      "    scales them down into a fixed range, where the largest occurring data point\n",
      "    corresponds to the maximum value and the smallest one corresponds to the\n",
      "    minimum value. For an example visualization, refer to :ref:`Compare\n",
      "    MinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.\n",
      "\n",
      "    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    feature_range : tuple (min, max), default=(0, 1)\n",
      "        Desired range of transformed data.\n",
      "\n",
      "    copy : bool, default=True\n",
      "        Set to False to perform inplace row normalization and avoid a\n",
      "        copy (if the input is already a numpy array).\n",
      "\n",
      "    clip : bool, default=False\n",
      "        Set to True to clip transformed values of held-out data to\n",
      "        provided `feature range`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    min_ : ndarray of shape (n_features,)\n",
      "        Per feature adjustment for minimum. Equivalent to\n",
      "        ``min - X.min(axis=0) * self.scale_``\n",
      "\n",
      "    scale_ : ndarray of shape (n_features,)\n",
      "        Per feature relative scaling of the data. Equivalent to\n",
      "        ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           *scale_* attribute.\n",
      "\n",
      "    data_min_ : ndarray of shape (n_features,)\n",
      "        Per feature minimum seen in the data\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           *data_min_*\n",
      "\n",
      "    data_max_ : ndarray of shape (n_features,)\n",
      "        Per feature maximum seen in the data\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           *data_max_*\n",
      "\n",
      "    data_range_ : ndarray of shape (n_features,)\n",
      "        Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           *data_range_*\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    n_samples_seen_ : int\n",
      "        The number of samples processed by the estimator.\n",
      "        It will be reset on new calls to fit, but increments across\n",
      "        ``partial_fit`` calls.\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    minmax_scale : Equivalent function without the estimator API.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "    transform.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.preprocessing import MinMaxScaler\n",
      "    >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      "    >>> scaler = MinMaxScaler()\n",
      "    >>> print(scaler.fit(data))\n",
      "    MinMaxScaler()\n",
      "    >>> print(scaler.data_max_)\n",
      "    [ 1. 18.]\n",
      "    >>> print(scaler.transform(data))\n",
      "    [[0.   0.  ]\n",
      "     [0.25 0.25]\n",
      "     [0.5  0.5 ]\n",
      "     [1.   1.  ]]\n",
      "    >>> print(scaler.transform([[2, 2]]))\n",
      "    [[1.5 0. ]]\n",
      "    \n",
      "Now building 'minmaxscaler'. Ensure that parameter values are correct (Docstring above).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a value for 'feature_range' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'copy' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'clip' (or an iterable like a list or range for grid search if applicable):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully tested all arguments,\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press any key to continue \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pipeline Creation Menu ---\n",
      "To build a pipeline, add at least one transformer and one predictor section.The last section must always be a predictor. Sections must be built before they can be added.\n",
      "\n",
      "Options:\n",
      "1. Build a transformer section\n",
      "   - A transformer applies preprocessing steps to your data, such as scaling, encoding, or imputation.\n",
      "2. Build a predictor\n",
      "   - A predictor is the final step in the pipeline, such as a regression model or classifier.\n",
      "3. Add a section to the pipeline\n",
      "   - Use this to integrate a previously defined transformer or predictor into the pipeline.\n",
      "4. View current pipeline\n",
      "   - Displays the steps currently added to this pipeline, in the order they will be applied.\n",
      "5. View current available sections\n",
      "   - Displays the steps currently added to this pipeline, in the order they will be applied.\n",
      "6. Finish and save this pipeline\n",
      "   - Completes the pipeline creation process and saves the current pipeline.\n",
      "7. Cancel this pipeline\n",
      "   - Discards the current pipeline and returns to the main menu.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building predictor.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the predictors class name:  RandomForestClassifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A random forest classifier.\n",
      "\n",
      "    A random forest is a meta estimator that fits a number of decision tree\n",
      "    classifiers on various sub-samples of the dataset and uses averaging to\n",
      "    improve the predictive accuracy and control over-fitting.\n",
      "    Trees in the forest use the best split strategy, i.e. equivalent to passing\n",
      "    `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\n",
      "    The sub-sample size is controlled with the `max_samples` parameter if\n",
      "    `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      "    each tree.\n",
      "\n",
      "    For a comparison between tree-based ensemble models see the example\n",
      "    :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
      "\n",
      "    Read more in the :ref:`User Guide <forest>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_estimators : int, default=100\n",
      "        The number of trees in the forest.\n",
      "\n",
      "        .. versionchanged:: 0.22\n",
      "           The default value of ``n_estimators`` changed from 10 to 100\n",
      "           in 0.22.\n",
      "\n",
      "    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
      "        The function to measure the quality of a split. Supported criteria are\n",
      "        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
      "        Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
      "        Note: This parameter is tree-specific.\n",
      "\n",
      "    max_depth : int, default=None\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "\n",
      "    min_samples_split : int or float, default=2\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a fraction and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_samples_leaf : int or float, default=1\n",
      "        The minimum number of samples required to be at a leaf node.\n",
      "        A split point at any depth will only be considered if it leaves at\n",
      "        least ``min_samples_leaf`` training samples in each of the left and\n",
      "        right branches.  This may have the effect of smoothing the model,\n",
      "        especially in regression.\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a fraction and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_weight_fraction_leaf : float, default=0.0\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "        - If int, then consider `max_features` features at each split.\n",
      "        - If float, then `max_features` is a fraction and\n",
      "          `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      "          split.\n",
      "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "        - If \"log2\", then `max_features=log2(n_features)`.\n",
      "        - If None, then `max_features=n_features`.\n",
      "\n",
      "        .. versionchanged:: 1.1\n",
      "            The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    max_leaf_nodes : int, default=None\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_decrease : float, default=0.0\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    bootstrap : bool, default=True\n",
      "        Whether bootstrap samples are used when building trees. If False, the\n",
      "        whole dataset is used to build each tree.\n",
      "\n",
      "    oob_score : bool or callable, default=False\n",
      "        Whether to use out-of-bag samples to estimate the generalization score.\n",
      "        By default, :func:`~sklearn.metrics.accuracy_score` is used.\n",
      "        Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
      "        custom metric. Only available if `bootstrap=True`.\n",
      "\n",
      "    n_jobs : int, default=None\n",
      "        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "        context. ``-1`` means using all processors. See :term:`Glossary\n",
      "        <n_jobs>` for more details.\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls both the randomness of the bootstrapping of the samples used\n",
      "        when building trees (if ``bootstrap=True``) and the sampling of the\n",
      "        features to consider when looking for the best split at each node\n",
      "        (if ``max_features < n_features``).\n",
      "        See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "    verbose : int, default=0\n",
      "        Controls the verbosity when fitting and predicting.\n",
      "\n",
      "    warm_start : bool, default=False\n",
      "        When set to ``True``, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "        new forest. See :term:`Glossary <warm_start>` and\n",
      "        :ref:`tree_ensemble_warm_start` for details.\n",
      "\n",
      "    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      "        Weights associated with classes in the form ``{class_label: weight}``.\n",
      "        If not given, all classes are supposed to have weight one. For\n",
      "        multi-output problems, a list of dicts can be provided in the same\n",
      "        order as the columns of y.\n",
      "\n",
      "        Note that for multioutput (including multilabel) weights should be\n",
      "        defined for each class of every column in its own dict. For example,\n",
      "        for four-class multilabel classification weights should be\n",
      "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "\n",
      "        The \"balanced\" mode uses the values of y to automatically adjust\n",
      "        weights inversely proportional to class frequencies in the input data\n",
      "        as ``n_samples / (n_classes * np.bincount(y))``\n",
      "\n",
      "        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      "        weights are computed based on the bootstrap sample for every tree\n",
      "        grown.\n",
      "\n",
      "        For multi-output, the weights of each column of y will be multiplied.\n",
      "\n",
      "        Note that these weights will be multiplied with sample_weight (passed\n",
      "        through the fit method) if sample_weight is specified.\n",
      "\n",
      "    ccp_alpha : non-negative float, default=0.0\n",
      "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "        subtree with the largest cost complexity that is smaller than\n",
      "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "        :ref:`minimal_cost_complexity_pruning` for details.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    max_samples : int or float, default=None\n",
      "        If bootstrap is True, the number of samples to draw from X\n",
      "        to train each base estimator.\n",
      "\n",
      "        - If None (default), then draw `X.shape[0]` samples.\n",
      "        - If int, then draw `max_samples` samples.\n",
      "        - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
      "          `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    monotonic_cst : array-like of int of shape (n_features), default=None\n",
      "        Indicates the monotonicity constraint to enforce on each feature.\n",
      "          - 1: monotonic increase\n",
      "          - 0: no constraint\n",
      "          - -1: monotonic decrease\n",
      "\n",
      "        If monotonic_cst is None, no constraints are applied.\n",
      "\n",
      "        Monotonicity constraints are not supported for:\n",
      "          - multiclass classifications (i.e. when `n_classes > 2`),\n",
      "          - multioutput classifications (i.e. when `n_outputs_ > 1`),\n",
      "          - classifications trained on data with missing values.\n",
      "\n",
      "        The constraints hold over the probability of the positive class.\n",
      "\n",
      "        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      "\n",
      "        .. versionadded:: 1.4\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n",
      "        The child estimator template used to create the collection of fitted\n",
      "        sub-estimators.\n",
      "\n",
      "        .. versionadded:: 1.2\n",
      "           `base_estimator_` was renamed to `estimator_`.\n",
      "\n",
      "    estimators_ : list of DecisionTreeClassifier\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      "        The classes labels (single output problem), or a list of arrays of\n",
      "        class labels (multi-output problem).\n",
      "\n",
      "    n_classes_ : int or list\n",
      "        The number of classes (single output problem), or a list containing the\n",
      "        number of classes for each output (multi-output problem).\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    n_outputs_ : int\n",
      "        The number of outputs when ``fit`` is performed.\n",
      "\n",
      "    feature_importances_ : ndarray of shape (n_features,)\n",
      "        The impurity-based feature importances.\n",
      "        The higher, the more important the feature.\n",
      "        The importance of a feature is computed as the (normalized)\n",
      "        total reduction of the criterion brought by that feature.  It is also\n",
      "        known as the Gini importance.\n",
      "\n",
      "        Warning: impurity-based feature importances can be misleading for\n",
      "        high cardinality features (many unique values). See\n",
      "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "\n",
      "    oob_score_ : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "        This attribute exists only when ``oob_score`` is True.\n",
      "\n",
      "    oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
      "        Decision function computed with out-of-bag estimate on the training\n",
      "        set. If n_estimators is small it might be possible that a data point\n",
      "        was never left out during the bootstrap. In this case,\n",
      "        `oob_decision_function_` might contain NaN. This attribute exists\n",
      "        only when ``oob_score`` is True.\n",
      "\n",
      "    estimators_samples_ : list of arrays\n",
      "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "        estimator. Each subset is defined by an array of the indices selected.\n",
      "\n",
      "        .. versionadded:: 1.4\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      "    sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
      "        tree classifiers.\n",
      "    sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n",
      "        Boosting Classification Tree, very fast for big datasets (n_samples >=\n",
      "        10_000).\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The default values for the parameters controlling the size of the trees\n",
      "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "    unpruned trees which can potentially be very large on some data sets. To\n",
      "    reduce memory consumption, the complexity and size of the trees should be\n",
      "    controlled by setting those parameter values.\n",
      "\n",
      "    The features are always randomly permuted at each split. Therefore,\n",
      "    the best found split may vary, even with the same training data,\n",
      "    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "    of the criterion is identical for several splits enumerated during the\n",
      "    search of the best split. To obtain a deterministic behaviour during\n",
      "    fitting, ``random_state`` has to be fixed.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.ensemble import RandomForestClassifier\n",
      "    >>> from sklearn.datasets import make_classification\n",
      "    >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      "    ...                            n_informative=2, n_redundant=0,\n",
      "    ...                            random_state=0, shuffle=False)\n",
      "    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      "    >>> clf.fit(X, y)\n",
      "    RandomForestClassifier(...)\n",
      "    >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "    [1]\n",
      "    \n",
      "Now building 'randomforestclassifier'. Ensure that parameter values are correct (Docstring above).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a value for 'n_estimators' (or an iterable like a list or range for grid search if applicable):  range(10,200, 10)\n",
      "Enter a value for 'criterion' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'max_depth' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'min_samples_split' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'min_samples_leaf' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'min_weight_fraction_leaf' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'max_features' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'max_leaf_nodes' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'min_impurity_decrease' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'bootstrap' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'oob_score' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'n_jobs' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'random_state' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'verbose' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'warm_start' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'class_weight' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'ccp_alpha' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'max_samples' (or an iterable like a list or range for grid search if applicable):  \n",
      "Enter a value for 'monotonic_cst' (or an iterable like a list or range for grid search if applicable):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with arguments: {'n_estimators': 10}\n",
      "Testing with arguments: {'n_estimators': 20}\n",
      "Testing with arguments: {'n_estimators': 30}\n",
      "Testing with arguments: {'n_estimators': 40}\n",
      "Testing with arguments: {'n_estimators': 50}\n",
      "Testing with arguments: {'n_estimators': 60}\n",
      "Testing with arguments: {'n_estimators': 70}\n",
      "Testing with arguments: {'n_estimators': 80}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with arguments: {'n_estimators': 90}\n",
      "Testing with arguments: {'n_estimators': 100}\n",
      "Testing with arguments: {'n_estimators': 110}\n",
      "Testing with arguments: {'n_estimators': 120}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with arguments: {'n_estimators': 130}\n",
      "Testing with arguments: {'n_estimators': 140}\n",
      "Testing with arguments: {'n_estimators': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with arguments: {'n_estimators': 160}\n",
      "Testing with arguments: {'n_estimators': 170}\n",
      "Testing with arguments: {'n_estimators': 180}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with arguments: {'n_estimators': 190}\n",
      "Testing with arguments: {'n_estimators': 190}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully tested all arguments,\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press any key to continue \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pipeline Creation Menu ---\n",
      "To build a pipeline, add at least one transformer and one predictor section.The last section must always be a predictor. Sections must be built before they can be added.\n",
      "\n",
      "Options:\n",
      "1. Build a transformer section\n",
      "   - A transformer applies preprocessing steps to your data, such as scaling, encoding, or imputation.\n",
      "2. Build a predictor\n",
      "   - A predictor is the final step in the pipeline, such as a regression model or classifier.\n",
      "3. Add a section to the pipeline\n",
      "   - Use this to integrate a previously defined transformer or predictor into the pipeline.\n",
      "4. View current pipeline\n",
      "   - Displays the steps currently added to this pipeline, in the order they will be applied.\n",
      "5. View current available sections\n",
      "   - Displays the steps currently added to this pipeline, in the order they will be applied.\n",
      "6. Finish and save this pipeline\n",
      "   - Completes the pipeline creation process and saves the current pipeline.\n",
      "7. Cancel this pipeline\n",
      "   - Discards the current pipeline and returns to the main menu.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding section to the pipeline.\n",
      "Select which type of section you wish to add to the pipeline\n",
      "1. Add a transformer\n",
      "2. Add a predictor\n",
      "3. Add a column_transformer (requires a transformer)\n",
      "4. Go back to Pipeline creation menu.\n",
      "transformers\n",
      "{'simpleimputer': {'name': 'simpleimputer', 'class': 'SimpleImputer', 'args': {'strategy': 'median'}, 'grid': {}, 'columns': []}, 'minmaxscaler': {'name': 'minmaxscaler', 'class': 'MinMaxScaler', 'args': {}, 'grid': {}, 'columns': []}}\n",
      "\n",
      "predictors\n",
      "{'randomforestclassifier': {'name': 'randomforestclassifier', 'class': 'RandomForestClassifier', 'args': {}, 'grid': {'n_estimators': 'range(10,200, 10)'}, 'columns': []}}\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sections: \n",
      "dict_keys(['simpleimputer', 'minmaxscaler'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select a transformer by entering its name (see above) simpleimputer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding section to the pipeline.\n",
      "Select which type of section you wish to add to the pipeline\n",
      "1. Add a transformer\n",
      "2. Add a predictor\n",
      "3. Add a column_transformer (requires a transformer)\n",
      "4. Go back to Pipeline creation menu.\n",
      "transformers\n",
      "{'minmaxscaler': {'name': 'minmaxscaler', 'class': 'MinMaxScaler', 'args': {}, 'grid': {}, 'columns': []}}\n",
      "\n",
      "predictors\n",
      "{'randomforestclassifier': {'name': 'randomforestclassifier', 'class': 'RandomForestClassifier', 'args': {}, 'grid': {'n_estimators': 'range(10,200, 10)'}, 'columns': []}}\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sections: \n",
      "dict_keys(['minmaxscaler'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select a transformer by entering its name (see above) minmaxscaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding section to the pipeline.\n",
      "Select which type of section you wish to add to the pipeline\n",
      "1. Add a transformer\n",
      "2. Add a predictor\n",
      "3. Add a column_transformer (requires a transformer)\n",
      "4. Go back to Pipeline creation menu.\n",
      "transformers\n",
      "{}\n",
      "\n",
      "predictors\n",
      "{'randomforestclassifier': {'name': 'randomforestclassifier', 'class': 'RandomForestClassifier', 'args': {}, 'grid': {'n_estimators': 'range(10,200, 10)'}, 'columns': []}}\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sections: \n",
      "dict_keys(['randomforestclassifier'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select a transformer by entering its name (see above) randomforestclassifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding section to the pipeline.\n",
      "Select which type of section you wish to add to the pipeline\n",
      "1. Add a transformer\n",
      "2. Add a predictor\n",
      "3. Add a column_transformer (requires a transformer)\n",
      "4. Go back to Pipeline creation menu.\n",
      "transformers\n",
      "{}\n",
      "\n",
      "predictors\n",
      "{}\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning to previous menu.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press any key to continue \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pipeline Creation Menu ---\n",
      "To build a pipeline, add at least one transformer and one predictor section.The last section must always be a predictor. Sections must be built before they can be added.\n",
      "\n",
      "Options:\n",
      "1. Build a transformer section\n",
      "   - A transformer applies preprocessing steps to your data, such as scaling, encoding, or imputation.\n",
      "2. Build a predictor\n",
      "   - A predictor is the final step in the pipeline, such as a regression model or classifier.\n",
      "3. Add a section to the pipeline\n",
      "   - Use this to integrate a previously defined transformer or predictor into the pipeline.\n",
      "4. View current pipeline\n",
      "   - Displays the steps currently added to this pipeline, in the order they will be applied.\n",
      "5. View current available sections\n",
      "   - Displays the steps currently added to this pipeline, in the order they will be applied.\n",
      "6. Finish and save this pipeline\n",
      "   - Completes the pipeline creation process and saves the current pipeline.\n",
      "7. Cancel this pipeline\n",
      "   - Discards the current pipeline and returns to the main menu.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice:  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the current pipeline with predictor: randomforestclassifier\n",
      "Pipeline 'randomforestclassifier' saved successfully.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Welcome to IterPlumber! Please select an option:\n",
      "    1. Create a new pipeline\n",
      "    2. View current pipelines\n",
      "    3. Finalize blueprints\n",
      "    4. Cancel and exit\n",
      "    Your choice: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finalizing and saving pipelines...\n",
      "\n",
      "Finalizing and saving all created pipelines...\n",
      "Signed pipeline configuration saved at: /home/nottoriousgg/MsC/MachineLearning/src/config/config_id_7.json\n",
      "Pipelines saved successfully.\n",
      "Pipelines finalized successfully. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Assuming IterPlumber expects the feature columns and optionally the target column:\n",
    "column_names = list(X)\n",
    "mario = IterPlumber(column_names=column_names, auditer=auditer)\n",
    "\n",
    "# Run pipes\n",
    "mario.run_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f759f72-a2ca-4dbc-aed0-af9523113f0c",
   "metadata": {},
   "source": [
    "## Fit data\n",
    "\n",
    "Consumes instructions, as pipelines, via gridsearch, cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4cba1a76-6da9-43c4-85f3-09de9b994884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('randomforestclassifier', Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n",
      "                ('minmaxscaler', MinMaxScaler()),\n",
      "                ('randomforestclassifier', RandomForestClassifier())]), {'randomforestclassifier__n_estimators': range(10, 200, 10)})]\n",
      "Fitting 3 folds for each of 19 candidates, totalling 57 fits\n",
      "[CV] END ............randomforestclassifier__n_estimators=10; total time=   0.2s\n",
      "[CV] END ............randomforestclassifier__n_estimators=10; total time=   0.2s\n",
      "[CV] END ............randomforestclassifier__n_estimators=10; total time=   0.2s\n",
      "[CV] END ............randomforestclassifier__n_estimators=20; total time=   0.3s\n",
      "[CV] END ............randomforestclassifier__n_estimators=20; total time=   0.3s\n",
      "[CV] END ............randomforestclassifier__n_estimators=20; total time=   0.3s\n",
      "[CV] END ............randomforestclassifier__n_estimators=30; total time=   0.4s\n",
      "[CV] END ............randomforestclassifier__n_estimators=30; total time=   0.4s\n",
      "[CV] END ............randomforestclassifier__n_estimators=30; total time=   0.4s\n",
      "[CV] END ............randomforestclassifier__n_estimators=40; total time=   0.5s\n",
      "[CV] END ............randomforestclassifier__n_estimators=40; total time=   0.5s\n",
      "[CV] END ............randomforestclassifier__n_estimators=40; total time=   0.5s\n",
      "[CV] END ............randomforestclassifier__n_estimators=50; total time=   0.6s\n",
      "[CV] END ............randomforestclassifier__n_estimators=50; total time=   0.6s\n",
      "[CV] END ............randomforestclassifier__n_estimators=50; total time=   0.6s\n",
      "[CV] END ............randomforestclassifier__n_estimators=60; total time=   0.7s\n",
      "[CV] END ............randomforestclassifier__n_estimators=60; total time=   0.7s\n",
      "[CV] END ............randomforestclassifier__n_estimators=60; total time=   0.7s\n",
      "[CV] END ............randomforestclassifier__n_estimators=70; total time=   0.8s\n",
      "[CV] END ............randomforestclassifier__n_estimators=70; total time=   0.8s\n",
      "[CV] END ............randomforestclassifier__n_estimators=70; total time=   0.8s\n",
      "[CV] END ............randomforestclassifier__n_estimators=80; total time=   1.0s\n",
      "[CV] END ............randomforestclassifier__n_estimators=80; total time=   1.0s\n",
      "[CV] END ............randomforestclassifier__n_estimators=80; total time=   0.9s\n",
      "[CV] END ............randomforestclassifier__n_estimators=90; total time=   1.1s\n",
      "[CV] END ............randomforestclassifier__n_estimators=90; total time=   1.1s\n",
      "[CV] END ............randomforestclassifier__n_estimators=90; total time=   1.1s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=100; total time=   1.4s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=100; total time=   1.3s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=100; total time=   1.2s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=110; total time=   1.3s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=110; total time=   1.3s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=110; total time=   1.3s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=120; total time=   1.4s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=120; total time=   1.4s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=120; total time=   1.4s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=130; total time=   1.6s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=130; total time=   1.5s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=130; total time=   1.6s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=140; total time=   1.8s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=140; total time=   1.7s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=140; total time=   1.8s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=150; total time=   2.0s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=150; total time=   1.8s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=150; total time=   1.8s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=160; total time=   1.9s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=160; total time=   1.8s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=160; total time=   1.9s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=170; total time=   2.1s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=170; total time=   2.4s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=170; total time=   2.3s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=180; total time=   2.1s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=180; total time=   2.3s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=180; total time=   2.0s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=190; total time=   2.2s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=190; total time=   2.1s\n",
      "[CV] END ...........randomforestclassifier__n_estimators=190; total time=   2.1s\n",
      "[CV] END .................................................... total time= 1.2min\n",
      "Fitting 3 folds for each of 19 candidates, totalling 57 fits\n",
      "[CV] END ............randomforestclassifier__n_estimators=10; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 370, in _score\n",
      "    response_method = _check_response_method(estimator, self._response_method)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nottoriousgg/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n",
      "    raise AttributeError(\n",
      "AttributeError: GridSearchCV has none of the following attributes: predict.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ............randomforestclassifier__n_estimators=10; total time=   0.2s\n",
      "[CV] END ............randomforestclassifier__n_estimators=10; total time=   0.2s\n",
      "[CV] END ............randomforestclassifier__n_estimators=20; total time=   0.3s\n",
      "[CV] END ............randomforestclassifier__n_estimators=20; total time=   0.3s\n",
      "[CV] END ............randomforestclassifier__n_estimators=20; total time=   0.3s\n",
      "[CV] END ............randomforestclassifier__n_estimators=30; total time=   0.4s\n",
      "[CV] END ............randomforestclassifier__n_estimators=30; total time=   0.4s\n",
      "[CV] END ............randomforestclassifier__n_estimators=30; total time=   0.4s\n",
      "[CV] END ............randomforestclassifier__n_estimators=40; total time=   0.5s\n",
      "[CV] END ............randomforestclassifier__n_estimators=40; total time=   0.5s\n",
      "[CV] END ............randomforestclassifier__n_estimators=40; total time=   0.5s\n",
      "[CV] END ............randomforestclassifier__n_estimators=50; total time=   0.6s\n",
      "[CV] END ............randomforestclassifier__n_estimators=50; total time=   0.7s\n",
      "[CV] END ............randomforestclassifier__n_estimators=50; total time=   0.6s\n",
      "[CV] END ............randomforestclassifier__n_estimators=60; total time=   0.7s\n",
      "[CV] END ............randomforestclassifier__n_estimators=60; total time=   0.7s\n",
      "[CV] END ............randomforestclassifier__n_estimators=60; total time=   0.8s\n",
      "[CV] END ............randomforestclassifier__n_estimators=70; total time=   0.8s\n",
      "[CV] END ............randomforestclassifier__n_estimators=70; total time=   0.9s\n",
      "[CV] END ............randomforestclassifier__n_estimators=70; total time=   0.9s\n",
      "[CV] END ............randomforestclassifier__n_estimators=80; total time=   1.0s\n",
      "[CV] END ............randomforestclassifier__n_estimators=80; total time=   0.9s\n",
      "[CV] END ............randomforestclassifier__n_estimators=80; total time=   1.0s\n",
      "[CV] END ............randomforestclassifier__n_estimators=90; total time=   1.1s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No result data to save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 124\u001b[0m, in \u001b[0;36mFitter.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m    115\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipeline,\n\u001b[1;32m    116\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparams,  \u001b[38;5;66;03m# Param grid from last step\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m         \n\u001b[1;32m    122\u001b[0m )\n\u001b[0;32m--> 124\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[1;32m    125\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mgrid_search,\n\u001b[1;32m    126\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX,\n\u001b[1;32m    127\u001b[0m     y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my,\n\u001b[1;32m    128\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mouter_cv,\n\u001b[1;32m    129\u001b[0m     return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    130\u001b[0m     return_estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    131\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    132\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    133\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    136\u001b[0m best_fold_index \u001b[38;5;241m=\u001b[39m cv_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39margmax()\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    425\u001b[0m         clone(estimator),\n\u001b[1;32m    426\u001b[0m         X,\n\u001b[1;32m    427\u001b[0m         y,\n\u001b[1;32m    428\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[1;32m    429\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    430\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    431\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    432\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    433\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[1;32m    434\u001b[0m         score_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore,\n\u001b[1;32m    435\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[1;32m    436\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    437\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[1;32m    438\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    439\u001b[0m     )\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[1;32m    441\u001b[0m )\n\u001b[1;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    961\u001b[0m         )\n\u001b[1;32m    962\u001b[0m     )\n\u001b[0;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    966\u001b[0m         clone(base_estimator),\n\u001b[1;32m    967\u001b[0m         X,\n\u001b[1;32m    968\u001b[0m         y,\n\u001b[1;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    975\u001b[0m     )\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m    979\u001b[0m     )\n\u001b[1;32m    980\u001b[0m )\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/imblearn/pipeline.py:329\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    328\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 329\u001b[0m Xt, yt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/imblearn/pipeline.py:255\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    253\u001b[0m     cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m ):\n\u001b[0;32m--> 255\u001b[0m     X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    256\u001b[0m         cloned_transformer,\n\u001b[1;32m    257\u001b[0m         X,\n\u001b[1;32m    258\u001b[0m         y,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    260\u001b[0m         message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    261\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[1;32m    262\u001b[0m         params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_resample\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/imblearn/pipeline.py:1104\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1104\u001b[0m     res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/impute/_base.py:421\u001b[0m, in \u001b[0;36mSimpleImputer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the imputer on `X`.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(X, in_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# otherwise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/impute/_base.py:332\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    333\u001b[0m         X,\n\u001b[1;32m    334\u001b[0m         reset\u001b[38;5;241m=\u001b[39min_fit,\n\u001b[1;32m    335\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    336\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    337\u001b[0m         force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m in_fit \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    338\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[1;32m    339\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1065\u001b[0m         array,\n\u001b[1;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1069\u001b[0m     )\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/sklearn/utils/validation.py:119\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 119\u001b[0m     first_pass_isfinite \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39misfinite(xp\u001b[38;5;241m.\u001b[39msum(X))\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[1;32m   2299\u001b[0m                       initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m fitter \u001b[38;5;241m=\u001b[39m Fitter(X_train, y_train, instructions_json\u001b[38;5;241m=\u001b[39minstructions, logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Run the fitting process\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m fitter\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[0;32mIn[74], line 140\u001b[0m, in \u001b[0;36mFitter.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_and_store_results(model_name, cv_results, best_fold_index)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mml_logger\u001b[38;5;241m.\u001b[39mresults_to_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_buffer)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll results saved by the logger.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 139\u001b[0m, in \u001b[0;36mMLLogger.results_to_csv\u001b[0;34m(self, result_data)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mAppends results to a CSV file, injecting the config_id and run_id.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result_data:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo result data to save.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(result_data)\n\u001b[1;32m    142\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id\n",
      "\u001b[0;31mValueError\u001b[0m: No result data to save."
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Auditer passes a valid instruction set \n",
    "instructions = auditer.get_target_dir_target_id(target_dir=\"config\", target_id=\"7\")\n",
    "\n",
    "# Creates an instance of the Logger\n",
    "logger = MLLogger(base_dir=base_dir, instructions_json=instructions)\n",
    "\n",
    "# Create the Fitter instance\n",
    "fitter = Fitter(X_train, y_train, instructions_json=instructions, logger=logger)\n",
    "\n",
    "# Run the fitting process\n",
    "fitter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9578c8-d54d-4e41-a651-471edccfc859",
   "metadata": {},
   "source": [
    "Fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
